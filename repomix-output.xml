This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  agents/
    data-hygiene-toolkit-developer.md
  settings.local.json
api/
  app/
    routes/
      auth/
        auth.py
      upload/
        upload.py
      users/
        users.py
      executions.py
      processing.py
      reports.py
      rules.py
    services/
      __init__.py
      data_import.py
      data_quality.py
      export.py
      rule_engine.py
    auth.py
    database.py
    main.py
    models.py
    schemas.py
  migrations/
    alembic/
      versions/
        04ca9eab8c98_create_employee_table.py
        f3f72ff529c3_create_comprehensive_data_hygiene_schema.py
      env.py
      README
      script.py.mako
    alembic.ini
.gitignore
.python-version
Dockerfile.fastapi
Dockerfile.migrations
pyproject.toml
UI_plan.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/agents/data-hygiene-toolkit-developer.md">
---
name: data-hygiene-toolkit-developer
description: Use this agent when working on the Data Hygiene Toolkit project and encountering issues or tasks related to: Error 500/404 responses, API endpoint development, CSV validation, rule engine functionality, ETL checks, database schema/migrations, Alembic operations, FastAPI bugs, Docker compose issues, frontend forms/tables, test coverage, authentication, Postgres indexing, seed data, or Angular UI components. Examples: <example>Context: User encounters a 500 error when uploading CSV files. user: 'I'm getting Error 500 when trying to upload a CSV file through the /upload endpoint' assistant: 'I'll use the data-hygiene-toolkit-developer agent to investigate and fix this CSV upload issue' <commentary>Since this involves Error 500 and CSV validation in the Data Hygiene Toolkit, use the specialized agent to diagnose and resolve the issue.</commentary></example> <example>Context: User needs to add a new rule validation feature. user: 'We need to add a new endpoint for previewing data quality rules before applying them' assistant: 'I'll use the data-hygiene-toolkit-developer agent to implement the rule preview functionality' <commentary>This involves rule engine work and API endpoint development, which are core responsibilities of this agent.</commentary></example>
model: inherit
---

You are a senior full-stack developer specializing in the Data Hygiene Toolkit, a three-tier application focused on data quality validation and rule execution. You own feature development, bug fixes, and testing across UI (Angular), API (FastAPI), and database (PostgreSQL) layers.

**Tech Stack Context:**
- Backend: FastAPI + Pydantic + uvicorn in app/ folder
- Database: PostgreSQL + Alembic migrations
- Frontend: Angular + TypeScript in ui/ folder
- Orchestration: Docker Compose

**Your Development Process:**

1. **Analysis Phase**: Read relevant files, FastAPI specs, and migrations. Summarize your understanding and assumptions before proceeding.

2. **Design Phase**: Create a minimal, focused plan that keeps changes small, safe, and reversible.

3. **Database Layer** (when needed):
   - Design schema changes that protect data quality (constraints, enums, FKs, indices)
   - Generate idempotent Alembic migrations with proper upgrade/downgrade
   - Prefer set-based SQL operations and proper indexing

4. **API Layer**:
   - Implement FastAPI routes with proper Pydantic models and validation
   - Ensure responses are typed, paginated, and use consistent status codes
   - Add comprehensive input validation and sanitization
   - Update OpenAPI documentation
   - Write unit tests (pytest) and integration tests

5. **Frontend Layer** (when needed):
   - Modify Angular components for rule creation, dataset upload, and results dashboards
   - Ensure type safety with TypeScript
   - Follow project linting standards

6. **Performance & Safety**:
   - Avoid N+1 queries; use efficient SQL patterns
   - Implement streaming for large files
   - Validate inputs at both UI and API levels
   - Apply size limits and sanitize file names/paths
   - Never hardcode secrets; use environment variables

7. **Testing & Verification**:
   - Use Docker Compose to run full stack
   - Provide exact commands for testing
   - Include before/after comparisons and sample payloads

**Code Quality Standards:**
- Keep PRs under 300 LOC when possible
- Write descriptive commit messages like 'feat(api): add /rules/preview' or 'db: add unique index on (dataset_id, rule_id)'
- Follow project formatting (ruff/black, eslint/prettier)
- Use composition over large functions
- Add docstrings for non-obvious logic
- Maintain type safety throughout

**Scope Boundaries:**
- You MAY modify: API routes/schemas/services, DB models/constraints/indices, UI components, dev scripts
- You MUST NOT: Introduce breaking API changes without migration guide, hardcode secrets, make heavy infrastructure changes

**Documentation Requirements:**
- Update README with setup instructions, environment variables, and new endpoints
- Add CHANGELOG entries
- Include rollback notes for risky features
- Update OpenAPI documentation with examples

**When to Ask for Clarification:**
- Breaking changes to schema or API
- Heavy infrastructure modifications
- Unclear CSV limits, file size caps, or rule semantics
- Authentication-related changes

Always prioritize data quality, user experience, and system reliability. Your solutions should be production-ready, well-tested, and easily reversible.
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(npm install:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="api/app/routes/auth/auth.py">
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.security import HTTPAuthorizationCredentials
from sqlalchemy.orm import Session
from datetime import timedelta

from app.database import get_session
from app.models import User, UserRole
from app.schemas import UserCreate, UserLogin, UserResponse, TokenResponse
from app.auth import (
    get_password_hash, 
    verify_password, 
    create_access_token, 
    get_current_user, 
    ACCESS_TOKEN_EXPIRE_MINUTES
)

router = APIRouter(prefix="/auth", tags=["Authentication"])

@router.post("/register", response_model=UserResponse)
async def register_user(
    user_data: UserCreate,
    db: Session = Depends(get_session)
):
    # Check if user already exists
    existing_user = db.query(User).filter(User.email == user_data.email).first()
    if existing_user:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Email already registered"
        )
    
    # Create new user
    hashed_password = get_password_hash(user_data.password)
    new_user = User(
        name=user_data.name,
        email=user_data.email,
        role=user_data.role,
        auth_provider="local",
        auth_subject=hashed_password
    )
    
    db.add(new_user)
    db.commit()
    db.refresh(new_user)
    
    return new_user

@router.post("/login", response_model=TokenResponse)
async def login_user(
    credentials: UserLogin,
    db: Session = Depends(get_session)
):
    # Find user by email
    user = db.query(User).filter(User.email == credentials.email).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid email or password"
        )
    
    # Verify password
    if not verify_password(credentials.password, user.auth_subject):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid email or password"
        )
    
    # Create access token
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.id, "email": user.email, "role": user.role.value},
        expires_delta=access_token_expires
    )
    
    return TokenResponse(
        access_token=access_token,
        token_type="bearer",
        user=UserResponse.model_validate(user)
    )

@router.get("/me", response_model=UserResponse)
async def get_current_user_info(
    current_user: User = Depends(get_current_user)
):
    return current_user

@router.get("/users", response_model=list[UserResponse])
async def list_users(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    # Only admins can list all users
    if current_user.role != UserRole.admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Only admins can list users"
        )
    
    users = db.query(User).all()
    return users

@router.put("/users/{user_id}/role")
async def update_user_role(
    user_id: str,
    new_role: UserRole,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_current_user)
):
    # Only admins can update user roles
    if current_user.role != UserRole.admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Only admins can update user roles"
        )
    
    user = db.query(User).filter(User.id == user_id).first()
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    
    user.role = new_role
    db.commit()
    db.refresh(user)
    
    return {"message": "User role updated successfully", "user": UserResponse.model_validate(user)}
</file>

<file path="api/app/routes/upload/upload.py">
from fastapi import APIRouter, Depends, UploadFile, File, Form, HTTPException, status
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional, cast
import json

from app.database import get_session
from app.models import User, Dataset
from app.auth import get_any_authenticated_user
from app.schemas import DatasetResponse, DataProfileResponse
from app.services.data_import import DataImportService

router = APIRouter(prefix="/data", tags=["Data Import"])


@router.post("/upload/file", response_model=Dict[str, Any])
async def upload_file(
    file: UploadFile = File(...),
    dataset_name: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Upload and process a CSV or Excel file
    """
    # Validate file size (limit to 50MB)
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

    if file.size is not None and file.size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail="File size exceeds 50MB limit"
        )

    # Validate file type
    allowed_extensions = ['.csv', '.xlsx', '.xls', '.txt']
    if file.filename is None:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="File must have a filename"
        )
    file_extension = '.' + file.filename.split('.')[-1].lower()

    if file_extension not in allowed_extensions:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"File type {file_extension} not supported. Allowed types: {', '.join(allowed_extensions)}"
        )

    # Process the file
    import_service = DataImportService(db)
    result = await import_service.import_file(file, current_user, dataset_name)

    return {
        "message": "File uploaded and processed successfully",
        "dataset": result['dataset'],
        "profile": result['profile']
    }


@router.post("/upload/json", response_model=Dict[str, Any])
async def upload_json_data(
    dataset_name: str,
    data: List[Dict[str, Any]],
    description: Optional[str] = None,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Upload JSON data directly
    """
    if not data:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="JSON data cannot be empty"
        )

    if len(data) > 100000:  # Limit to 100k records
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail="JSON data exceeds 100,000 records limit"
        )

    # Process the JSON data
    import_service = DataImportService(db)
    result = import_service.import_json_data(data, current_user, dataset_name)

    return {
        "message": "JSON data processed successfully",
        "dataset": result['dataset'],
        "profile": result['profile']
    }


@router.get("/datasets", response_model=List[DatasetResponse])
async def list_datasets(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    List all datasets accessible to the current user
    """
    datasets = db.query(Dataset).all()
    return [DatasetResponse.model_validate(dataset) for dataset in datasets]


@router.get("/datasets/{dataset_id}", response_model=DatasetResponse)
async def get_dataset(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get details of a specific dataset
    """
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    return DatasetResponse.model_validate(dataset)


@router.delete("/datasets/{dataset_id}")
async def delete_dataset(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Delete a dataset (only by owner or admin)
    """
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    # Check permissions
    from app.models import UserRole
    # Cast to Dataset type to help type checker
    dataset = cast(Dataset, dataset)
    uploaded_by_id = str(getattr(dataset, 'uploaded_by', ''))
    current_user_id = str(current_user.id)
    user_role = getattr(current_user, 'role', None)
    if uploaded_by_id != current_user_id and user_role != UserRole.admin:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="You don't have permission to delete this dataset"
        )

    db.delete(dataset)
    db.commit()

    return {"message": "Dataset deleted successfully"}
</file>

<file path="api/app/routes/users/users.py">
from pydantic import BaseModel, Field


class User(BaseModel):
    name: str = Field(..., min_length=3, max_length=20)
    userid: str = Field(..., min_length=3, max_length=20)
    userType: str
    password: str = Field(..., min_length=6, max_length=20)
</file>

<file path="api/app/routes/executions.py">
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional
import json

from app.database import get_session
from app.models import (
    User, Execution, ExecutionStatus, DatasetVersion,
    Issue, ExecutionRule, Dataset
)
from app.auth import get_any_authenticated_user, get_admin_user
from app.schemas import (
    ExecutionResponse, ExecutionCreate, IssueResponse
)
from app.services.rule_engine import RuleEngineService

router = APIRouter(prefix="/executions", tags=["Rule Executions"])


@router.get("/", response_model=List[ExecutionResponse])
async def list_executions(
    limit: int = Query(20, ge=1, le=100),
    status_filter: Optional[ExecutionStatus] = Query(None, description="Filter by execution status"),
    dataset_id: Optional[str] = Query(None, description="Filter by dataset ID"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    List recent rule executions with optional filtering
    """
    query = db.query(Execution)

    if status_filter:
        query = query.filter(Execution.status == status_filter)

    if dataset_id:
        query = query.join(DatasetVersion).filter(DatasetVersion.dataset_id == dataset_id)

    executions = query.order_by(Execution.started_at.desc()).limit(limit).all()
    return [ExecutionResponse.model_validate(execution) for execution in executions]


@router.get("/{execution_id}", response_model=ExecutionResponse)
async def get_execution(
    execution_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get details of a specific execution
    """
    execution = db.query(Execution).filter(Execution.id == execution_id).first()
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found"
        )

    return ExecutionResponse.model_validate(execution)


@router.post("/", response_model=ExecutionResponse)
async def create_execution(
    execution_data: ExecutionCreate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Execute rules on a dataset version
    """
    # Get dataset version
    dataset_version = db.query(DatasetVersion).filter(
        DatasetVersion.id == execution_data.dataset_version_id
    ).first()

    if not dataset_version:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset version not found"
        )

    # Check if dataset is accessible by user (you might want to add access control)

    try:
        rule_service = RuleEngineService(db)
        execution = rule_service.execute_rules_on_dataset(
            dataset_version=dataset_version,
            rule_ids=execution_data.rule_ids,
            current_user=current_user
        )

        return ExecutionResponse.model_validate(execution)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error executing rules: {str(e)}"
        )


@router.get("/{execution_id}/issues", response_model=List[IssueResponse])
async def get_execution_issues(
    execution_id: str,
    limit: int = Query(100, ge=1, le=1000),
    severity: Optional[str] = Query(None, description="Filter by issue severity"),
    category: Optional[str] = Query(None, description="Filter by issue category"),
    resolved: Optional[bool] = Query(None, description="Filter by resolution status"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get issues found during a specific execution
    """
    execution = db.query(Execution).filter(Execution.id == execution_id).first()
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found"
        )

    query = db.query(Issue).filter(Issue.execution_id == execution_id)

    if severity:
        query = query.filter(Issue.severity == severity)

    if category:
        query = query.filter(Issue.category == category)

    if resolved is not None:
        query = query.filter(Issue.resolved == resolved)

    issues = query.order_by(Issue.created_at.desc()).limit(limit).all()

    return [IssueResponse.model_validate(issue) for issue in issues]


@router.get("/{execution_id}/summary")
async def get_execution_summary(
    execution_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get summary statistics for an execution
    """
    execution = db.query(Execution).filter(Execution.id == execution_id).first()
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found"
        )

    # Get execution rules with their stats
    execution_rules = db.query(ExecutionRule).filter(
        ExecutionRule.execution_id == execution_id
    ).all()

    # Get issues breakdown
    issues = db.query(Issue).filter(Issue.execution_id == execution_id).all()

    # Calculate summary statistics
    issues_by_severity = {}
    issues_by_category = {}
    issues_by_rule = {}

    for issue in issues:
        # Count by severity
        severity = issue.severity.value if hasattr(issue.severity, 'value') else str(issue.severity)
        issues_by_severity[severity] = issues_by_severity.get(severity, 0) + 1

        # Count by category
        category = issue.category or 'unknown'
        issues_by_category[category] = issues_by_category.get(category, 0) + 1

        # Count by rule
        rule_id = issue.rule_id
        issues_by_rule[rule_id] = issues_by_rule.get(rule_id, 0) + 1

    return {
        "execution_id": execution_id,
        "status": execution.status.value if hasattr(execution.status, 'value') else str(execution.status),
        "total_rules": execution.total_rules,
        "total_rows": execution.total_rows,
        "rows_affected": execution.rows_affected,
        "columns_affected": execution.columns_affected,
        "total_issues": len(issues),
        "issues_by_severity": issues_by_severity,
        "issues_by_category": issues_by_category,
        "issues_by_rule": issues_by_rule,
        "rule_performance": [
            {
                "rule_id": er.rule_id,
                "error_count": er.error_count,
                "rows_flagged": er.rows_flagged,
                "cols_flagged": er.cols_flagged,
                "note": er.note
            }
            for er in execution_rules
        ],
        "started_at": execution.started_at,
        "finished_at": execution.finished_at,
        "duration_seconds": (
            (execution.finished_at - execution.started_at).total_seconds()
            if execution.finished_at and execution.started_at
            else None
        )
    }


@router.delete("/{execution_id}")
async def cancel_execution(
    execution_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)  # Only admins can cancel executions
):
    """
    Cancel a running execution (only for running executions)
    """
    execution = db.query(Execution).filter(Execution.id == execution_id).first()
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found"
        )

    if execution.status != ExecutionStatus.running:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Cannot cancel execution with status: {execution.status}"
        )

    # Update execution status to cancelled
    execution.status = ExecutionStatus.failed  # Assuming no 'cancelled' status exists
    execution.finished_at = db.execute("SELECT NOW()").scalar()

    # Update summary with cancellation info
    current_summary = json.loads(execution.summary) if execution.summary else {}
    current_summary['cancelled_by'] = current_user.id
    current_summary['cancellation_reason'] = 'Manual cancellation'
    execution.summary = json.dumps(current_summary)

    db.commit()

    return {"message": "Execution cancelled successfully"}


@router.get("/{execution_id}/rules", response_model=List[Dict[str, Any]])
async def get_execution_rules(
    execution_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get performance details for each rule in an execution
    """
    execution = db.query(Execution).filter(Execution.id == execution_id).first()
    if not execution:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Execution not found"
        )

    execution_rules = db.query(ExecutionRule).filter(
        ExecutionRule.execution_id == execution_id
    ).all()

    result = []
    for er in execution_rules:
        # Get rule details
        rule = er.rule if hasattr(er, 'rule') else None

        rule_info = {
            "rule_id": er.rule_id,
            "rule_name": rule.name if rule else "Unknown",
            "rule_kind": rule.kind.value if rule and hasattr(rule.kind, 'value') else str(rule.kind) if rule else "Unknown",
            "error_count": er.error_count,
            "rows_flagged": er.rows_flagged,
            "cols_flagged": er.cols_flagged,
            "note": er.note,
            "issues_found": db.query(Issue).filter(
                Issue.execution_id == execution_id,
                Issue.rule_id == er.rule_id
            ).count()
        }
        result.append(rule_info)

    return result
</file>

<file path="api/app/routes/processing.py">
from fastapi import APIRouter, Depends, HTTPException, status, Query, Body
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional
import json

from app.database import get_session
from app.models import (
    User, Dataset, DatasetVersion, DatasetStatus, Issue, Fix, Execution
)
from app.auth import get_any_authenticated_user, get_admin_user
from app.schemas import (
    DatasetResponse, FixCreate, FixResponse, IssueResponse
)
from app.services.data_quality import DataQualityService
from app.services.data_import import DataImportService

router = APIRouter(prefix="/processing", tags=["Data Processing"])


@router.post("/datasets/{dataset_id}/validate")
async def validate_dataset(
    dataset_id: str,
    missing_data_strategy: Optional[str] = Query("smart", description="Missing data handling strategy"),
    standardization_rules: Optional[Dict[str, str]] = Body(None, description="Column standardization rules"),
    validation_rules: Optional[Dict[str, Dict[str, Any]]] = Body(None, description="Value validation rules"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Validate and clean a dataset using data quality algorithms

    Args:
        dataset_id: ID of dataset to process
        missing_data_strategy: Strategy for handling missing data ('drop', 'mean', 'median', 'mode', 'forward_fill', 'smart')
        standardization_rules: Dict mapping columns to standardization types ('date', 'phone', 'email', 'address', 'name', 'currency')
        validation_rules: Dict mapping columns to validation configurations

    Returns:
        Processing results and new dataset version information
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    # Check permissions (user must be uploader, admin, or analyst)
    if (current_user.role.value not in ["admin", "analyst"] and
        dataset.uploaded_by != current_user.id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions to process this dataset"
        )

    try:
        data_quality_service = DataQualityService(db)
        data_import_service = DataImportService(db)

        # Get the latest dataset version
        latest_version = (
            db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .order_by(DatasetVersion.version_number.desc())
            .first()
        )

        if not latest_version:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No dataset version found"
            )

        # Load the dataset
        df = data_import_service.load_dataset_file(dataset_id, latest_version.version_number)
        original_df = df.copy()

        processing_report = {
            "dataset_id": dataset_id,
            "original_version": latest_version.version_number,
            "original_rows": len(df),
            "original_columns": len(df.columns),
            "processing_steps": []
        }

        # Step 1: Handle missing data
        if missing_data_strategy and missing_data_strategy != "none":
            df, missing_report = data_quality_service.handle_missing_data(
                df, strategy=missing_data_strategy
            )
            processing_report["processing_steps"].append({
                "step": "missing_data_handling",
                "strategy": missing_data_strategy,
                "report": missing_report
            })

        # Step 2: Standardize data
        if standardization_rules:
            df, standardization_report = data_quality_service.standardize_data(
                df, standardization_rules
            )
            processing_report["processing_steps"].append({
                "step": "data_standardization",
                "rules": standardization_rules,
                "report": standardization_report
            })

        # Step 3: Validate values
        if validation_rules:
            df, validation_report = data_quality_service.validate_values(
                df, validation_rules
            )
            processing_report["processing_steps"].append({
                "step": "value_validation",
                "rules": validation_rules,
                "report": validation_report
            })

        # Save processed dataset as new version
        new_version_number = latest_version.version_number + 1
        file_path = data_import_service.save_dataset_file(
            dataset_id, df, new_version_number
        )

        # Create new dataset version record
        new_version = DatasetVersion(
            dataset_id=dataset_id,
            version_number=new_version_number,
            file_path=file_path,
            row_count=len(df),
            column_count=len(df.columns),
            notes=f"Data quality processing applied with {len(processing_report['processing_steps'])} steps"
        )
        db.add(new_version)

        # Update dataset status
        dataset.status = DatasetStatus.validated
        db.commit()

        processing_report.update({
            "new_version": new_version_number,
            "final_rows": len(df),
            "final_columns": len(df.columns),
            "rows_changed": len(df) - len(original_df),
            "processing_success": True
        })

        return processing_report

    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Data processing failed: {str(e)}"
        )


@router.get("/datasets/{dataset_id}/compare-versions")
async def compare_dataset_versions(
    dataset_id: str,
    version1: int = Query(..., description="First version number to compare"),
    version2: int = Query(..., description="Second version number to compare"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Compare two versions of a dataset to show differences
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    try:
        data_import_service = DataImportService(db)

        # Load both versions
        df1 = data_import_service.load_dataset_file(dataset_id, version1)
        df2 = data_import_service.load_dataset_file(dataset_id, version2)

        comparison = {
            "dataset_id": dataset_id,
            "version1": version1,
            "version2": version2,
            "size_comparison": {
                "version1_rows": len(df1),
                "version1_columns": len(df1.columns),
                "version2_rows": len(df2),
                "version2_columns": len(df2.columns),
                "row_difference": len(df2) - len(df1),
                "column_difference": len(df2.columns) - len(df1.columns)
            },
            "column_changes": {},
            "data_quality_comparison": {}
        }

        # Compare columns
        v1_columns = set(df1.columns)
        v2_columns = set(df2.columns)

        comparison["column_changes"] = {
            "added_columns": list(v2_columns - v1_columns),
            "removed_columns": list(v1_columns - v2_columns),
            "common_columns": list(v1_columns & v2_columns)
        }

        # Compare data quality metrics for common columns
        for column in comparison["column_changes"]["common_columns"]:
            if column in df1.columns and column in df2.columns:
                comparison["data_quality_comparison"][column] = {
                    "missing_data": {
                        "version1": df1[column].isnull().sum(),
                        "version2": df2[column].isnull().sum(),
                        "improvement": df1[column].isnull().sum() - df2[column].isnull().sum()
                    },
                    "unique_values": {
                        "version1": df1[column].nunique(),
                        "version2": df2[column].nunique()
                    }
                }

        return comparison

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Version comparison failed: {str(e)}"
        )


@router.get("/datasets/{dataset_id}/quality-summary")
async def get_quality_summary(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get comprehensive data quality summary for a dataset
    """
    try:
        data_quality_service = DataQualityService(db)
        summary = data_quality_service.create_data_quality_summary(dataset_id)
        return summary

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate quality summary: {str(e)}"
        )


@router.post("/issues/{issue_id}/fix")
async def create_fix(
    issue_id: str,
    fix_data: FixCreate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Create a fix for a specific data quality issue
    """
    # Check if issue exists
    issue = db.query(Issue).filter(Issue.id == issue_id).first()
    if not issue:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Issue not found"
        )

    # Create fix record
    fix = Fix(
        issue_id=issue_id,
        fixed_by=current_user.id,
        new_value=fix_data.new_value,
        comment=fix_data.comment
    )

    db.add(fix)
    db.commit()
    db.refresh(fix)

    return FixResponse.model_validate(fix)


@router.get("/issues/{issue_id}/fixes", response_model=List[FixResponse])
async def get_issue_fixes(
    issue_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get all fixes applied to a specific issue
    """
    fixes = db.query(Fix).filter(Fix.issue_id == issue_id).all()
    return [FixResponse.model_validate(fix) for fix in fixes]


@router.post("/datasets/{dataset_id}/apply-corrections")
async def apply_corrections(
    dataset_id: str,
    corrections: List[Dict[str, Any]] = Body(..., description="List of corrections to apply"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Apply manual corrections to dataset data

    Args:
        dataset_id: Dataset to correct
        corrections: List of correction objects with fields:
                    - row_index: Row index to correct
                    - column: Column name to correct
                    - new_value: New value to set
                    - issue_id: (optional) Related issue ID
    """
    # Check if dataset exists and user has permissions
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    if (current_user.role.value not in ["admin", "analyst"] and
        dataset.uploaded_by != current_user.id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions to modify this dataset"
        )

    try:
        data_quality_service = DataQualityService(db)
        result = data_quality_service.apply_corrections(
            dataset_id, corrections, current_user.id
        )

        # Update dataset status if corrections were applied
        if result["corrections_applied"] > 0:
            dataset.status = DatasetStatus.cleaned
            db.commit()

        return result

    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to apply corrections: {str(e)}"
        )


@router.get("/datasets/{dataset_id}/processing-history")
async def get_processing_history(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get the processing history for a dataset including all versions and changes
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    # Get all versions
    versions = (
        db.query(DatasetVersion)
        .filter(DatasetVersion.dataset_id == dataset_id)
        .order_by(DatasetVersion.version_number.asc())
        .all()
    )

    # Get all executions for this dataset
    executions = (
        db.query(Execution)
        .filter(Execution.dataset_version_id.in_([v.id for v in versions]))
        .order_by(Execution.created_at.asc())
        .all()
    )

    # Get all fixes for issues from these executions
    execution_ids = [e.id for e in executions]
    fixes = (
        db.query(Fix)
        .join(Issue)
        .filter(Issue.execution_id.in_(execution_ids))
        .all()
    )

    history = {
        "dataset_id": dataset_id,
        "dataset_name": dataset.name,
        "current_status": dataset.status.value,
        "total_versions": len(versions),
        "versions": [
            {
                "version_number": version.version_number,
                "created_at": version.created_at,
                "row_count": version.row_count,
                "column_count": version.column_count,
                "notes": version.notes
            }
            for version in versions
        ],
        "executions": [
            {
                "id": execution.id,
                "created_at": execution.created_at,
                "status": execution.status.value,
                "rules_executed": execution.rules_executed,
                "issues_found": execution.issues_found,
                "duration_seconds": execution.duration_seconds
            }
            for execution in executions
        ],
        "fixes_applied": len(fixes),
        "timeline": []
    }

    # Create timeline combining versions, executions, and fixes
    timeline_events = []

    for version in versions:
        timeline_events.append({
            "timestamp": version.created_at,
            "type": "version_created",
            "version_number": version.version_number,
            "description": f"Version {version.version_number} created: {version.notes or 'No notes'}"
        })

    for execution in executions:
        timeline_events.append({
            "timestamp": execution.created_at,
            "type": "rule_execution",
            "execution_id": execution.id,
            "description": f"Rules executed: {execution.issues_found} issues found"
        })

    for fix in fixes:
        timeline_events.append({
            "timestamp": fix.fixed_at,
            "type": "issue_fixed",
            "fix_id": fix.id,
            "description": f"Issue fixed: {fix.comment or 'Manual correction'}"
        })

    # Sort timeline by timestamp
    timeline_events.sort(key=lambda x: x["timestamp"])
    history["timeline"] = timeline_events

    return history


@router.post("/datasets/{dataset_id}/bulk-corrections")
async def apply_bulk_corrections(
    dataset_id: str,
    correction_rules: Dict[str, Dict[str, Any]] = Body(..., description="Bulk correction rules by column"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Apply bulk corrections using automated rules

    Args:
        dataset_id: Dataset to correct
        correction_rules: Dict with column names as keys and correction configs as values
                         Example: {
                             "email": {"type": "standardize", "method": "lowercase"},
                             "phone": {"type": "standardize", "method": "international"},
                             "missing_data": {"strategy": "median"}
                         }
    """
    # Check permissions
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    if (current_user.role.value not in ["admin", "analyst"] and
        dataset.uploaded_by != current_user.id):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Insufficient permissions to modify this dataset"
        )

    try:
        data_quality_service = DataQualityService(db)
        data_import_service = DataImportService(db)

        # Get latest version
        latest_version = (
            db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .order_by(DatasetVersion.version_number.desc())
            .first()
        )

        # Load dataset
        df = data_import_service.load_dataset_file(dataset_id, latest_version.version_number)
        original_df = df.copy()

        bulk_report = {
            "dataset_id": dataset_id,
            "original_version": latest_version.version_number,
            "rules_applied": [],
            "corrections_summary": {}
        }

        # Extract different types of correction rules
        standardization_rules = {}
        missing_data_strategy = None

        for column, rule_config in correction_rules.items():
            rule_type = rule_config.get("type", "")

            if rule_type == "standardize":
                method = rule_config.get("method")
                if method in ["date", "phone", "email", "address", "name", "currency"]:
                    standardization_rules[column] = method
            elif rule_type == "missing_data":
                missing_data_strategy = rule_config.get("strategy", "smart")

        # Apply missing data corrections
        if missing_data_strategy:
            df, missing_report = data_quality_service.handle_missing_data(
                df, strategy=missing_data_strategy
            )
            bulk_report["rules_applied"].append({
                "type": "missing_data",
                "strategy": missing_data_strategy,
                "report": missing_report
            })

        # Apply standardization corrections
        if standardization_rules:
            df, standardization_report = data_quality_service.standardize_data(
                df, standardization_rules
            )
            bulk_report["rules_applied"].append({
                "type": "standardization",
                "rules": standardization_rules,
                "report": standardization_report
            })

        # Save as new version if changes were made
        if len(bulk_report["rules_applied"]) > 0:
            new_version_number = latest_version.version_number + 1
            file_path = data_import_service.save_dataset_file(
                dataset_id, df, new_version_number
            )

            new_version = DatasetVersion(
                dataset_id=dataset_id,
                version_number=new_version_number,
                file_path=file_path,
                row_count=len(df),
                column_count=len(df.columns),
                notes=f"Bulk corrections applied: {len(bulk_report['rules_applied'])} rule types"
            )
            db.add(new_version)

            # Update dataset status
            dataset.status = DatasetStatus.cleaned
            db.commit()

            bulk_report["new_version"] = new_version_number
        else:
            bulk_report["new_version"] = None

        bulk_report["corrections_summary"] = {
            "original_rows": len(original_df),
            "final_rows": len(df),
            "rows_changed": len(df) - len(original_df),
            "total_rule_types_applied": len(bulk_report["rules_applied"])
        }

        return bulk_report

    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Bulk corrections failed: {str(e)}"
        )
</file>

<file path="api/app/routes/reports.py">
from fastapi import APIRouter, Depends, HTTPException, status, Query, Response
from fastapi.responses import FileResponse, StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional
import json
from pathlib import Path

from app.database import get_session
from app.models import (
    User, Dataset, DatasetVersion, Export, ExportFormat, Issue, Fix, Execution
)
from app.auth import get_any_authenticated_user, get_admin_user
from app.schemas import ExportCreate, ExportResponse
from app.services.export import ExportService
from app.services.data_quality import DataQualityService

router = APIRouter(prefix="/reports", tags=["Reports & Export"])


@router.post("/datasets/{dataset_id}/export")
async def export_dataset(
    dataset_id: str,
    export_format: ExportFormat = Query(..., description="Export format"),
    include_metadata: bool = Query(True, description="Include dataset metadata"),
    include_issues: bool = Query(False, description="Include identified issues"),
    execution_id: Optional[str] = Query(None, description="Specific execution context"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Export a dataset in the specified format

    Args:
        dataset_id: Dataset to export
        export_format: Format for export (csv, excel, json)
        include_metadata: Whether to include dataset metadata
        include_issues: Whether to include data quality issues
        execution_id: Optional execution ID for context

    Returns:
        Export information with download details
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    # Get latest version
    latest_version = (
        db.query(DatasetVersion)
        .filter(DatasetVersion.dataset_id == dataset_id)
        .order_by(DatasetVersion.version_number.desc())
        .first()
    )

    if not latest_version:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No dataset version found"
        )

    try:
        export_service = ExportService(db)
        export_id, file_path = export_service.export_dataset(
            dataset_version_id=latest_version.id,
            export_format=export_format,
            user_id=current_user.id,
            execution_id=execution_id,
            include_metadata=include_metadata,
            include_issues=include_issues
        )

        return {
            "export_id": export_id,
            "dataset_id": dataset_id,
            "dataset_name": dataset.name,
            "version_number": latest_version.version_number,
            "export_format": export_format.value,
            "file_path": file_path,
            "include_metadata": include_metadata,
            "include_issues": include_issues,
            "download_url": f"/reports/exports/{export_id}/download"
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Export failed: {str(e)}"
        )


@router.get("/datasets/{dataset_id}/export-history")
async def get_export_history(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get export history for a dataset
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    try:
        export_service = ExportService(db)
        history = export_service.get_export_history(dataset_id)

        return {
            "dataset_id": dataset_id,
            "dataset_name": dataset.name,
            "total_exports": len(history),
            "exports": history
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get export history: {str(e)}"
        )


@router.get("/exports/{export_id}/download")
async def download_export(
    export_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Download an exported file
    """
    try:
        export_service = ExportService(db)
        file_path, download_filename = export_service.get_export_file(export_id)

        # Check if file exists
        if not Path(file_path).exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Export file not found"
            )

        # Determine media type
        if file_path.endswith('.csv'):
            media_type = 'text/csv'
        elif file_path.endswith('.xlsx'):
            media_type = 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        elif file_path.endswith('.json'):
            media_type = 'application/json'
        elif file_path.endswith('.zip'):
            media_type = 'application/zip'
        else:
            media_type = 'application/octet-stream'

        return FileResponse(
            path=file_path,
            filename=download_filename,
            media_type=media_type
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Download failed: {str(e)}"
        )


@router.delete("/exports/{export_id}")
async def delete_export(
    export_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Delete an export and its associated file
    """
    try:
        export_service = ExportService(db)
        success = export_service.delete_export(export_id, current_user.id)

        return {
            "export_id": export_id,
            "deleted": success,
            "message": "Export deleted successfully"
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete export: {str(e)}"
        )


@router.post("/datasets/{dataset_id}/quality-report")
async def generate_quality_report(
    dataset_id: str,
    include_charts: bool = Query(False, description="Include visual charts (future feature)"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Generate comprehensive data quality report for a dataset
    """
    # Check if dataset exists
    dataset = db.query(Dataset).filter(Dataset.id == dataset_id).first()
    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    try:
        export_service = ExportService(db)
        export_id, file_path = export_service.export_data_quality_report(
            dataset_id=dataset_id,
            user_id=current_user.id,
            include_charts=include_charts
        )

        return {
            "export_id": export_id,
            "dataset_id": dataset_id,
            "dataset_name": dataset.name,
            "report_type": "data_quality_report",
            "file_path": file_path,
            "download_url": f"/reports/exports/{export_id}/download",
            "include_charts": include_charts
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Quality report generation failed: {str(e)}"
        )


@router.get("/datasets/{dataset_id}/quality-summary")
async def get_quality_summary(
    dataset_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get real-time data quality summary for a dataset
    """
    try:
        data_quality_service = DataQualityService(db)
        summary = data_quality_service.create_data_quality_summary(dataset_id)
        return summary

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate quality summary: {str(e)}"
        )


@router.get("/dashboard/overview")
async def get_dashboard_overview(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get overview statistics for the dashboard
    """
    try:
        # Basic counts
        total_datasets = db.query(Dataset).count()
        total_executions = db.query(Execution).count()
        total_issues = db.query(Issue).count()
        total_fixes = db.query(Fix).count()

        # Recent activity
        recent_datasets = (
            db.query(Dataset)
            .order_by(Dataset.uploaded_at.desc())
            .limit(5)
            .all()
        )

        recent_executions = (
            db.query(Execution)
            .order_by(Execution.created_at.desc())
            .limit(5)
            .all()
        )

        # Quality statistics
        datasets = db.query(Dataset).all()
        quality_scores = []

        for dataset in datasets:
            try:
                data_quality_service = DataQualityService(db)
                summary = data_quality_service.create_data_quality_summary(dataset.id)
                quality_scores.append(summary.get("data_quality_score", 0))
            except:
                continue

        avg_quality_score = sum(quality_scores) / len(quality_scores) if quality_scores else 0

        # Dataset status distribution
        status_distribution = {}
        for dataset in datasets:
            status = dataset.status.value
            status_distribution[status] = status_distribution.get(status, 0) + 1

        return {
            "overview": {
                "total_datasets": total_datasets,
                "total_executions": total_executions,
                "total_issues": total_issues,
                "total_fixes": total_fixes,
                "avg_quality_score": round(avg_quality_score, 2),
                "issues_fixed_rate": round((total_fixes / total_issues * 100) if total_issues > 0 else 0, 2)
            },
            "recent_activity": {
                "recent_datasets": [
                    {
                        "id": dataset.id,
                        "name": dataset.name,
                        "status": dataset.status.value,
                        "uploaded_at": dataset.uploaded_at
                    }
                    for dataset in recent_datasets
                ],
                "recent_executions": [
                    {
                        "id": execution.id,
                        "dataset_version_id": execution.dataset_version_id,
                        "status": execution.status.value,
                        "issues_found": execution.issues_found or 0,
                        "created_at": execution.created_at
                    }
                    for execution in recent_executions
                ]
            },
            "statistics": {
                "dataset_status_distribution": status_distribution,
                "quality_score_distribution": {
                    "excellent": len([s for s in quality_scores if s >= 90]),
                    "good": len([s for s in quality_scores if 70 <= s < 90]),
                    "fair": len([s for s in quality_scores if 50 <= s < 70]),
                    "poor": len([s for s in quality_scores if s < 50])
                }
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate dashboard overview: {str(e)}"
        )


@router.get("/analytics/quality-trends")
async def get_quality_trends(
    days: int = Query(30, description="Number of days to analyze"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get data quality trends over time
    """
    from datetime import datetime, timedelta

    try:
        # Get executions from the last N days
        start_date = datetime.now() - timedelta(days=days)

        executions = (
            db.query(Execution)
            .filter(Execution.created_at >= start_date)
            .order_by(Execution.created_at.asc())
            .all()
        )

        # Group by date
        trends = {}
        for execution in executions:
            date_key = execution.created_at.date().isoformat()

            if date_key not in trends:
                trends[date_key] = {
                    "date": date_key,
                    "total_executions": 0,
                    "total_issues": 0,
                    "successful_executions": 0,
                    "avg_execution_time": 0
                }

            trends[date_key]["total_executions"] += 1
            trends[date_key]["total_issues"] += execution.issues_found or 0

            if execution.status.value == "succeeded":
                trends[date_key]["successful_executions"] += 1

            if execution.duration_seconds:
                current_avg = trends[date_key]["avg_execution_time"]
                count = trends[date_key]["total_executions"]
                trends[date_key]["avg_execution_time"] = (
                    (current_avg * (count - 1) + execution.duration_seconds) / count
                )

        # Calculate success rates
        for trend in trends.values():
            total = trend["total_executions"]
            successful = trend["successful_executions"]
            trend["success_rate"] = (successful / total * 100) if total > 0 else 0

        return {
            "analysis_period": {
                "start_date": start_date.date().isoformat(),
                "end_date": datetime.now().date().isoformat(),
                "days_analyzed": days
            },
            "trends": list(trends.values()),
            "summary": {
                "total_executions": len(executions),
                "total_issues_found": sum(e.issues_found or 0 for e in executions),
                "avg_issues_per_execution": (
                    sum(e.issues_found or 0 for e in executions) / len(executions)
                    if executions else 0
                ),
                "overall_success_rate": (
                    len([e for e in executions if e.status.value == "succeeded"]) / len(executions) * 100
                    if executions else 0
                )
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate quality trends: {str(e)}"
        )


@router.get("/analytics/issue-patterns")
async def get_issue_patterns(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Analyze patterns in data quality issues
    """
    try:
        # Get all issues
        issues = db.query(Issue).all()

        if not issues:
            return {
                "message": "No issues found for analysis",
                "patterns": {}
            }

        # Analyze patterns
        patterns = {
            "by_severity": {},
            "by_column": {},
            "by_rule_type": {},
            "most_common_issues": [],
            "fix_rates": {}
        }

        # Group by severity
        for issue in issues:
            severity = issue.severity.value if issue.severity else "unknown"
            patterns["by_severity"][severity] = patterns["by_severity"].get(severity, 0) + 1

        # Group by column
        for issue in issues:
            column = issue.column_name or "unknown"
            patterns["by_column"][column] = patterns["by_column"].get(column, 0) + 1

        # Get rule information for issues
        for issue in issues:
            if issue.rule_id:
                from app.models import Rule
                rule = db.query(Rule).filter(Rule.id == issue.rule_id).first()
                if rule:
                    rule_type = rule.kind.value
                    patterns["by_rule_type"][rule_type] = patterns["by_rule_type"].get(rule_type, 0) + 1

        # Most common issue descriptions
        description_counts = {}
        for issue in issues:
            desc = issue.description or "No description"
            description_counts[desc] = description_counts.get(desc, 0) + 1

        patterns["most_common_issues"] = [
            {"description": desc, "count": count}
            for desc, count in sorted(description_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        ]

        # Fix rates by severity
        for severity in patterns["by_severity"]:
            severity_issues = [i for i in issues if (i.severity.value if i.severity else "unknown") == severity]
            severity_fixes = db.query(Fix).filter(
                Fix.issue_id.in_([i.id for i in severity_issues])
            ).count()

            total_severity_issues = len(severity_issues)
            fix_rate = (severity_fixes / total_severity_issues * 100) if total_severity_issues > 0 else 0
            patterns["fix_rates"][severity] = round(fix_rate, 2)

        return {
            "total_issues_analyzed": len(issues),
            "patterns": patterns,
            "insights": {
                "most_problematic_columns": sorted(
                    patterns["by_column"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5],
                "most_common_rule_violations": sorted(
                    patterns["by_rule_type"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to analyze issue patterns: {str(e)}"
        )


@router.get("/system/health")
async def get_system_health(
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)  # Admin only
):
    """
    Get system health metrics (admin only)
    """
    try:
        # Database health
        total_tables = {
            "users": db.query(User).count(),
            "datasets": db.query(Dataset).count(),
            "dataset_versions": db.query(DatasetVersion).count(),
            "executions": db.query(Execution).count(),
            "issues": db.query(Issue).count(),
            "fixes": db.query(Fix).count(),
            "exports": db.query(Export).count()
        }

        # Storage health
        from app.services.data_import import DATASET_STORAGE_PATH
        storage_path = Path(DATASET_STORAGE_PATH)

        if storage_path.exists():
            dataset_files = list(storage_path.glob("*.parquet"))
            total_storage_size = sum(f.stat().st_size for f in dataset_files)
        else:
            dataset_files = []
            total_storage_size = 0

        export_storage_path = Path("data/exports")
        if export_storage_path.exists():
            export_files = list(export_storage_path.glob("*"))
            export_storage_size = sum(f.stat().st_size for f in export_files if f.is_file())
        else:
            export_files = []
            export_storage_size = 0

        # Recent activity health
        from datetime import datetime, timedelta
        recent_threshold = datetime.now() - timedelta(hours=24)

        recent_activity = {
            "recent_uploads": db.query(Dataset).filter(Dataset.uploaded_at >= recent_threshold).count(),
            "recent_executions": db.query(Execution).filter(Execution.created_at >= recent_threshold).count(),
            "recent_exports": db.query(Export).filter(Export.created_at >= recent_threshold).count()
        }

        return {
            "system_status": "healthy",
            "timestamp": datetime.now(),
            "database_health": {
                "total_records": total_tables,
                "connection_status": "connected"
            },
            "storage_health": {
                "dataset_files_count": len(dataset_files),
                "total_dataset_storage_mb": round(total_storage_size / (1024 * 1024), 2),
                "export_files_count": len(export_files),
                "total_export_storage_mb": round(export_storage_size / (1024 * 1024), 2),
                "storage_paths": {
                    "datasets": str(storage_path),
                    "exports": str(export_storage_path)
                }
            },
            "activity_health": recent_activity,
            "performance_metrics": {
                "avg_execution_time": self._get_avg_execution_time(db),
                "success_rate": self._get_success_rate(db)
            }
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get system health: {str(e)}"
        )

def _get_avg_execution_time(db: Session) -> float:
    """Get average execution time from recent executions"""
    from datetime import datetime, timedelta

    recent_threshold = datetime.now() - timedelta(days=7)
    recent_executions = (
        db.query(Execution)
        .filter(Execution.created_at >= recent_threshold)
        .filter(Execution.duration_seconds.isnot(None))
        .all()
    )

    if not recent_executions:
        return 0.0

    total_time = sum(e.duration_seconds for e in recent_executions)
    return round(total_time / len(recent_executions), 2)

def _get_success_rate(db: Session) -> float:
    """Get success rate from recent executions"""
    from datetime import datetime, timedelta

    recent_threshold = datetime.now() - timedelta(days=7)
    recent_executions = (
        db.query(Execution)
        .filter(Execution.created_at >= recent_threshold)
        .all()
    )

    if not recent_executions:
        return 100.0

    successful = len([e for e in recent_executions if e.status.value == "succeeded"])
    return round(successful / len(recent_executions) * 100, 2)
</file>

<file path="api/app/routes/rules.py">
from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from typing import List, Dict, Any, Optional
import json

from app.database import get_session
from app.models import User, Rule, RuleKind, Criticality, Execution, Issue
from app.auth import get_any_authenticated_user, get_admin_user
from app.schemas import (
    RuleResponse, RuleCreate, RuleUpdate, ExecutionResponse, 
    IssueResponse, RuleTestRequest
)
from app.services.rule_engine import RuleEngineService

router = APIRouter(prefix="/rules", tags=["Business Rules"])


@router.get("/", response_model=List[RuleResponse])
async def list_rules(
    active_only: bool = Query(True, description="Filter to active rules only"),
    rule_kind: Optional[RuleKind] = Query(None, description="Filter by rule kind"),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    List all business rules with optional filtering
    """
    query = db.query(Rule)
    
    if active_only:
        query = query.filter(Rule.is_active == True)
    
    if rule_kind:
        query = query.filter(Rule.kind == rule_kind)
    
    rules = query.order_by(Rule.created_at.desc()).all()
    return [RuleResponse.model_validate(rule) for rule in rules]


@router.get("/{rule_id}", response_model=RuleResponse)
async def get_rule(
    rule_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get details of a specific rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    return RuleResponse.model_validate(rule)


@router.post("/", response_model=RuleResponse)
async def create_rule(
    rule_data: RuleCreate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)  # Only admins can create rules
):
    """
    Create a new business rule
    """
    rule_service = RuleEngineService(db)
    
    try:
        rule = rule_service.create_rule(
            name=rule_data.name,
            description=rule_data.description,
            kind=rule_data.kind,
            criticality=rule_data.criticality,
            target_columns=rule_data.target_columns,
            params=rule_data.params,
            current_user=current_user
        )
        
        return RuleResponse.model_validate(rule)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error creating rule: {str(e)}"
        )


@router.put("/{rule_id}", response_model=RuleResponse)
async def update_rule(
    rule_id: str,
    rule_data: RuleUpdate,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)
):
    """
    Update an existing rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    # Update fields if provided
    update_data = rule_data.model_dump(exclude_unset=True)
    
    for field, value in update_data.items():
        if field == 'target_columns':
            setattr(rule, field, json.dumps(value))
        elif field == 'params':
            setattr(rule, field, json.dumps(value))
        else:
            setattr(rule, field, value)
    
    db.commit()
    db.refresh(rule)
    
    return RuleResponse.model_validate(rule)


@router.patch("/{rule_id}/activate")
async def activate_rule(
    rule_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)
):
    """
    Activate a rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    rule.is_active = True
    db.commit()
    
    return {"message": "Rule activated successfully"}


@router.patch("/{rule_id}/deactivate")
async def deactivate_rule(
    rule_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)
):
    """
    Deactivate a rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    rule.is_active = False
    db.commit()
    
    return {"message": "Rule deactivated successfully"}


@router.delete("/{rule_id}")
async def delete_rule(
    rule_id: str,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_admin_user)
):
    """
    Delete a rule (only if no executions exist)
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    # Check if rule has been used in any executions
    executions_count = db.query(Execution).join(
        Execution.execution_rules
    ).filter_by(rule_id=rule_id).count()
    
    if executions_count > 0:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="Cannot delete rule that has been used in executions. Deactivate instead."
        )
    
    db.delete(rule)
    db.commit()
    
    return {"message": "Rule deleted successfully"}


@router.post("/{rule_id}/test", response_model=Dict[str, Any])
async def test_rule(
    rule_id: str,
    test_data: RuleTestRequest,
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Test a rule against sample data
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    try:
        import pandas as pd
        from app.services.rule_engine import RuleEngineService
        
        # Convert test data to DataFrame
        df = pd.DataFrame(test_data.sample_data)
        
        # Get appropriate validator
        rule_service = RuleEngineService(db)
        validator_class = rule_service.validators.get(rule.kind)
        
        if not validator_class:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"No validator available for rule kind: {rule.kind}"
            )
        
        # Run validation
        validator = validator_class(rule, df, db)
        issues = validator.validate()
        
        return {
            "rule_name": rule.name,
            "total_rows_tested": len(df),
            "issues_found": len(issues),
            "sample_issues": issues[:10],  # Return first 10 issues
            "summary": {
                "rows_with_issues": len(set(issue['row_index'] for issue in issues)),
                "columns_with_issues": len(set(issue['column_name'] for issue in issues)),
                "categories": list(set(issue['category'] for issue in issues))
            }
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error testing rule: {str(e)}"
        )


@router.get("/kinds/available", response_model=List[Dict[str, str]])
async def get_available_rule_kinds():
    """
    Get list of available rule kinds with descriptions
    """
    return [
        {
            "kind": "missing_data",
            "description": "Detect missing or null values in required fields",
            "example_params": {
                "columns": ["column1", "column2"],
                "default_value": ""
            }
        },
        {
            "kind": "standardization", 
            "description": "Standardize data formats (dates, phones, emails)",
            "example_params": {
                "columns": ["date_column"],
                "type": "date",
                "format": "%Y-%m-%d"
            }
        },
        {
            "kind": "value_list",
            "description": "Validate values against allowed list",
            "example_params": {
                "columns": ["status"],
                "allowed_values": ["active", "inactive"],
                "case_sensitive": True
            }
        },
        {
            "kind": "length_range",
            "description": "Validate field length constraints",
            "example_params": {
                "columns": ["description"],
                "min_length": 5,
                "max_length": 100
            }
        },
        {
            "kind": "char_restriction",
            "description": "Restrict to specific character types",
            "example_params": {
                "columns": ["name"],
                "type": "alphabetic"
            }
        },
        {
            "kind": "cross_field",
            "description": "Validate relationships between multiple fields",
            "example_params": {
                "rules": [
                    {
                        "type": "dependency",
                        "dependent_field": "state",
                        "required_field": "country"
                    }
                ]
            }
        },
        {
            "kind": "regex",
            "description": "Validate using regular expression patterns",
            "example_params": {
                "columns": ["email"],
                "patterns": [
                    {
                        "pattern": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
                        "name": "email_format",
                        "must_match": True
                    }
                ]
            }
        },
        {
            "kind": "custom",
            "description": "Custom validation using expressions or lookup tables",
            "example_params": {
                "type": "python_expression",
                "expression": "age >= 18",
                "columns": ["age"],
                "error_message": "Age must be 18 or older"
            }
        }
    ]


@router.get("/{rule_id}/executions", response_model=List[ExecutionResponse])
async def get_rule_executions(
    rule_id: str,
    limit: int = Query(10, ge=1, le=100),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get execution history for a specific rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    executions = db.query(Execution).join(
        Execution.execution_rules
    ).filter_by(rule_id=rule_id).order_by(
        Execution.started_at.desc()
    ).limit(limit).all()
    
    return [ExecutionResponse.model_validate(execution) for execution in executions]


@router.get("/{rule_id}/issues", response_model=List[IssueResponse])
async def get_rule_issues(
    rule_id: str,
    resolved: Optional[bool] = Query(None, description="Filter by resolution status"),
    limit: int = Query(50, ge=1, le=1000),
    db: Session = Depends(get_session),
    current_user: User = Depends(get_any_authenticated_user)
):
    """
    Get issues found by a specific rule
    """
    rule = db.query(Rule).filter(Rule.id == rule_id).first()
    if not rule:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Rule not found"
        )
    
    query = db.query(Issue).filter(Issue.rule_id == rule_id)
    
    if resolved is not None:
        query = query.filter(Issue.resolved == resolved)
    
    issues = query.order_by(Issue.created_at.desc()).limit(limit).all()
    
    return [IssueResponse.model_validate(issue) for issue in issues]
</file>

<file path="api/app/services/__init__.py">
# Services package
</file>

<file path="api/app/services/data_import.py">
import pandas as pd
import json
import hashlib
import uuid
import os
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from fastapi import UploadFile, HTTPException, status
from sqlalchemy.orm import Session
from io import BytesIO

from app.models import Dataset, DatasetVersion, DatasetColumn, SourceType, DatasetStatus, User
from app.schemas import DatasetResponse, DatasetColumnResponse, DataProfileResponse

# Configuration for data storage
DATASET_STORAGE_PATH = Path("data/datasets")


class DataImportService:

    def __init__(self, db: Session):
        self.db = db
        # Ensure storage directory exists
        DATASET_STORAGE_PATH.mkdir(parents=True, exist_ok=True)

    def calculate_file_checksum(self, content: bytes) -> str:
        """Calculate MD5 checksum of file content"""
        return hashlib.md5(content).hexdigest()

    def save_dataset_file(self, dataset_id: str, df: pd.DataFrame, version_no: int = 1) -> str:
        """Save dataset DataFrame to file storage and return the file path"""
        # Create filename: dataset_id_v{version_no}.parquet
        filename = f"{dataset_id}_v{version_no}.parquet"
        file_path = DATASET_STORAGE_PATH / filename

        # Save as parquet for efficient storage and fast loading
        df.to_parquet(file_path, index=False)

        return str(file_path)

    def load_dataset_file(self, dataset_id: str, version_no: int = 1) -> pd.DataFrame:
        """Load dataset DataFrame from file storage"""
        filename = f"{dataset_id}_v{version_no}.parquet"
        file_path = DATASET_STORAGE_PATH / filename

        if not file_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Dataset file not found: {filename}"
            )

        return pd.read_parquet(file_path)

    def detect_source_type(self, filename: str) -> SourceType:
        """Detect source type based on file extension"""
        ext = filename.lower().split('.')[-1]
        if ext in ['csv', 'txt']:
            return SourceType.csv
        elif ext in ['xlsx', 'xls']:
            return SourceType.excel
        elif ext == 'json':
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="JSON files should be processed through the JSON import endpoint"
            )
        else:
            return SourceType.other

    def read_file_to_dataframe(self, file_content: bytes, source_type: SourceType, filename: str) -> pd.DataFrame:
        """Convert file content to pandas DataFrame"""
        try:
            if source_type == SourceType.csv:
                # Try different encodings
                for encoding in ['utf-8', 'latin-1', 'cp1252']:
                    try:
                        df = pd.read_csv(
                            BytesIO(file_content), encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail="Could not decode CSV file with any supported encoding"
                    )

            elif source_type == SourceType.excel:
                df = pd.read_excel(BytesIO(file_content))

            else:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Unsupported file type for {filename}"
                )

            return df

        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Error processing file {filename}: {str(e)}"
            )

    def infer_column_types(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """Infer data types for each column"""
        column_info = []

        for i, column in enumerate(df.columns):
            series = df[column]

            # Calculate missing values
            null_count = series.isnull().sum()
            is_nullable = null_count > 0

            # Infer data type
            dtype_str = str(series.dtype)

            # More detailed type inference
            if dtype_str.startswith('int'):
                inferred_type = 'integer'
            elif dtype_str.startswith('float'):
                inferred_type = 'decimal'
            elif dtype_str == 'bool':
                inferred_type = 'boolean'
            elif dtype_str == 'datetime64':
                inferred_type = 'datetime'
            else:
                inferred_type = 'text'

                # Try to detect more specific types for object columns
                if dtype_str == 'object':
                    non_null_series = series.dropna()
                    if len(non_null_series) > 0:
                        # Check if it looks like dates
                        try:
                            pd.to_datetime(non_null_series.head(10))
                            inferred_type = 'datetime'
                        except:
                            # Check if it looks like numbers
                            try:
                                pd.to_numeric(non_null_series.head(10))
                                inferred_type = 'decimal'
                            except:
                                inferred_type = 'text'

            column_info.append({
                'name': column,
                'ordinal_position': i + 1,
                'inferred_type': inferred_type,
                'is_nullable': is_nullable,
                'null_count': int(null_count),
                'unique_count': int(series.nunique()),
                'sample_values': series.dropna().head(5).tolist()
            })

        return column_info

    def create_dataset_record(
        self,
        filename: str,
        df: pd.DataFrame,
        current_user: User,
        dataset_name: Optional[str] = None
    ) -> Dataset:
        """Create dataset record in database"""

        # Generate checksum
        file_content = df.to_csv(index=False).encode()
        checksum = self.calculate_file_checksum(file_content)

        # Check for existing dataset with same checksum
        existing_dataset = self.db.query(Dataset).filter(
            Dataset.checksum == checksum).first()
        if existing_dataset:
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"Dataset with identical content already exists: {existing_dataset.name}"
            )

        source_type = self.detect_source_type(filename)

        # Create dataset record
        dataset = Dataset(
            name=dataset_name or filename.split('.')[0],
            source_type=source_type,
            original_filename=filename,
            checksum=checksum,
            uploaded_by=current_user.id,
            status=DatasetStatus.uploaded,
            row_count=len(df),
            column_count=len(df.columns)
        )

        self.db.add(dataset)
        self.db.commit()
        self.db.refresh(dataset)

        return dataset

    def create_dataset_columns(self, dataset: Dataset, column_info: List[Dict[str, Any]]):
        """Create column records for dataset"""

        dataset_columns = []
        for col_info in column_info:
            column = DatasetColumn(
                dataset_id=dataset.id,
                name=col_info['name'],
                ordinal_position=col_info['ordinal_position'],
                inferred_type=col_info['inferred_type'],
                is_nullable=col_info['is_nullable']
            )
            dataset_columns.append(column)

        self.db.add_all(dataset_columns)
        self.db.commit()

        return dataset_columns

    def create_initial_version(self, dataset: Dataset, current_user: User) -> DatasetVersion:
        """Create initial version of dataset"""

        version = DatasetVersion(
            dataset_id=dataset.id,
            version_no=1,
            created_by=current_user.id,
            rows=dataset.row_count,
            columns=dataset.column_count,
            change_note="Initial dataset upload"
        )

        self.db.add(version)
        self.db.commit()
        self.db.refresh(version)

        return version

    async def import_file(
        self,
        file: UploadFile,
        current_user: User,
        dataset_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """Main method to import a file"""

        # Read file content
        content = await file.read()

        # Detect source type
        if not file.filename:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Uploaded file must have a filename"
            )
        source_type = self.detect_source_type(file.filename)

        # Convert to DataFrame
        df = self.read_file_to_dataframe(content, source_type, file.filename)

        # Validate DataFrame
        if df.empty:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="File appears to be empty or contains no data"
            )

        # Infer column types and get profile
        column_info = self.infer_column_types(df)

        # Create dataset record
        dataset = self.create_dataset_record(
            file.filename, df, current_user, dataset_name)

        # Create column records
        columns = self.create_dataset_columns(dataset, column_info)

        # Create initial version
        version = self.create_initial_version(dataset, current_user)

        # Save dataset data to file storage
        try:
            file_path = self.save_dataset_file(dataset.id, df, version.version_no)
        except Exception as e:
            # If file save fails, rollback database changes
            self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to save dataset file: {str(e)}"
            )

        # Update dataset status
        dataset.status = DatasetStatus.profiled
        self.db.commit()

        return {
            'dataset': DatasetResponse.model_validate(dataset),
            'profile': DataProfileResponse(
                total_rows=len(df),
                total_columns=len(df.columns),
                columns=[DatasetColumnResponse.model_validate(
                    col) for col in columns],
                data_types_summary={
                    col_info['inferred_type']: sum(
                        1 for c in column_info if c['inferred_type'] == col_info['inferred_type'])
                    for col_info in column_info
                },
                missing_values_summary={
                    col_info['name']: col_info['null_count']
                    for col_info in column_info
                    if col_info['null_count'] > 0
                }
            )
        }

    def import_json_data(
        self,
        json_data: List[Dict[str, Any]],
        current_user: User,
        dataset_name: str
    ) -> Dict[str, Any]:
        """Import JSON data directly"""

        if not json_data:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="JSON data is empty"
            )

        # Convert JSON to DataFrame
        try:
            df = pd.DataFrame(json_data)
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Error converting JSON to DataFrame: {str(e)}"
            )

        # Generate a dummy filename for JSON data
        filename = f"{dataset_name}.json"

        # Infer column types and get profile
        column_info = self.infer_column_types(df)

        # Create dataset record (but with JSON source type)
        file_content = json.dumps(json_data).encode()
        checksum = self.calculate_file_checksum(file_content)

        dataset = Dataset(
            name=dataset_name,
            source_type=SourceType.other,  # or create a JSON source type
            original_filename=filename,
            checksum=checksum,
            uploaded_by=current_user.id,
            status=DatasetStatus.uploaded,
            row_count=len(df),
            column_count=len(df.columns)
        )

        self.db.add(dataset)
        self.db.commit()
        self.db.refresh(dataset)

        # Create column records
        columns = self.create_dataset_columns(dataset, column_info)

        # Create initial version
        version = self.create_initial_version(dataset, current_user)

        # Save dataset data to file storage
        try:
            file_path = self.save_dataset_file(dataset.id, df, version.version_no)
        except Exception as e:
            # If file save fails, rollback database changes
            self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to save dataset file: {str(e)}"
            )

        # Update dataset status
        dataset.status = DatasetStatus.profiled
        self.db.commit()

        return {
            'dataset': DatasetResponse.model_validate(dataset),
            'profile': DataProfileResponse(
                total_rows=len(df),
                total_columns=len(df.columns),
                columns=[DatasetColumnResponse.model_validate(
                    col) for col in columns],
                data_types_summary={
                    col_info['inferred_type']: sum(
                        1 for c in column_info if c['inferred_type'] == col_info['inferred_type'])
                    for col_info in column_info
                },
                missing_values_summary={
                    col_info['name']: col_info['null_count']
                    for col_info in column_info
                    if col_info['null_count'] > 0
                }
            )
        }
</file>

<file path="api/app/services/data_quality.py">
import pandas as pd
import numpy as np
import re
import datetime
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from sqlalchemy.orm import Session
from fastapi import HTTPException, status

from app.models import (
    Dataset, DatasetVersion, DatasetColumn, Issue, Fix, Execution,
    DatasetStatus, User, SourceType
)
from app.schemas import FixCreate, FixResponse
from app.services.data_import import DataImportService


class DataQualityService:
    """
    Service for comprehensive data quality operations including:
    - Missing data handling
    - Data standardization
    - Value validation
    - Data correction workflows
    """

    def __init__(self, db: Session):
        self.db = db
        self.data_import_service = DataImportService(db)

    # === MISSING DATA HANDLING ===

    def handle_missing_data(
        self,
        df: pd.DataFrame,
        strategy: str = "smart",
        column_strategies: Optional[Dict[str, str]] = None
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        Handle missing data with various strategies

        Args:
            df: DataFrame to process
            strategy: Global strategy ('drop', 'mean', 'median', 'mode', 'forward_fill', 'smart')
            column_strategies: Column-specific strategies override

        Returns:
            Tuple of (cleaned_df, report)
        """
        cleaned_df = df.copy()
        report = {
            "original_missing_count": df.isnull().sum().to_dict(),
            "actions_taken": {},
            "final_missing_count": {},
            "rows_dropped": 0
        }

        # Apply column-specific strategies first
        if column_strategies:
            for column, col_strategy in column_strategies.items():
                if column in cleaned_df.columns:
                    cleaned_df, action = self._apply_missing_strategy(
                        cleaned_df, column, col_strategy
                    )
                    report["actions_taken"][column] = action

        # Apply global strategy to remaining columns with missing data
        for column in cleaned_df.columns:
            if column not in (column_strategies or {}) and cleaned_df[column].isnull().any():
                cleaned_df, action = self._apply_missing_strategy(
                    cleaned_df, column, strategy
                )
                if column not in report["actions_taken"]:
                    report["actions_taken"][column] = action

        report["final_missing_count"] = cleaned_df.isnull().sum().to_dict()
        report["rows_dropped"] = len(df) - len(cleaned_df)

        return cleaned_df, report

    def _apply_missing_strategy(
        self,
        df: pd.DataFrame,
        column: str,
        strategy: str
    ) -> Tuple[pd.DataFrame, str]:
        """Apply specific missing data strategy to a column"""

        if strategy == "drop":
            original_len = len(df)
            df = df.dropna(subset=[column])
            return df, f"Dropped {original_len - len(df)} rows with missing values"

        elif strategy == "mean" and df[column].dtype in ['int64', 'float64']:
            mean_val = df[column].mean()
            df[column] = df[column].fillna(mean_val)
            return df, f"Filled with mean value: {mean_val:.2f}"

        elif strategy == "median" and df[column].dtype in ['int64', 'float64']:
            median_val = df[column].median()
            df[column] = df[column].fillna(median_val)
            return df, f"Filled with median value: {median_val:.2f}"

        elif strategy == "mode":
            mode_val = df[column].mode()
            if not mode_val.empty:
                df[column] = df[column].fillna(mode_val.iloc[0])
                return df, f"Filled with mode value: {mode_val.iloc[0]}"

        elif strategy == "forward_fill":
            df[column] = df[column].fillna(method='ffill')
            return df, "Forward filled missing values"

        elif strategy == "smart":
            # Smart strategy: choose best method based on data type and distribution
            if df[column].dtype in ['int64', 'float64']:
                # For numeric: use median if skewed, mean if normal
                if abs(df[column].skew()) > 1:
                    return self._apply_missing_strategy(df, column, "median")
                else:
                    return self._apply_missing_strategy(df, column, "mean")
            else:
                # For categorical: use mode
                return self._apply_missing_strategy(df, column, "mode")

        return df, "No action taken"

    # === DATA STANDARDIZATION ===

    def standardize_data(
        self,
        df: pd.DataFrame,
        standardization_rules: Dict[str, str]
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        Standardize data formats across columns

        Args:
            df: DataFrame to standardize
            standardization_rules: Dict mapping column names to standardization types
                                 ('date', 'phone', 'email', 'address', 'name', 'currency')

        Returns:
            Tuple of (standardized_df, report)
        """
        standardized_df = df.copy()
        report = {
            "columns_processed": [],
            "standardization_actions": {},
            "errors": {}
        }

        for column, rule_type in standardization_rules.items():
            if column not in standardized_df.columns:
                report["errors"][column] = f"Column not found in dataset"
                continue

            try:
                if rule_type == "date":
                    standardized_df[column], action = self._standardize_dates(
                        standardized_df[column]
                    )
                elif rule_type == "phone":
                    standardized_df[column], action = self._standardize_phones(
                        standardized_df[column]
                    )
                elif rule_type == "email":
                    standardized_df[column], action = self._standardize_emails(
                        standardized_df[column]
                    )
                elif rule_type == "address":
                    standardized_df[column], action = self._standardize_addresses(
                        standardized_df[column]
                    )
                elif rule_type == "name":
                    standardized_df[column], action = self._standardize_names(
                        standardized_df[column]
                    )
                elif rule_type == "currency":
                    standardized_df[column], action = self._standardize_currency(
                        standardized_df[column]
                    )
                else:
                    action = f"Unknown standardization type: {rule_type}"

                report["columns_processed"].append(column)
                report["standardization_actions"][column] = action

            except Exception as e:
                report["errors"][column] = str(e)

        return standardized_df, report

    def _standardize_dates(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize date formats to ISO 8601 (YYYY-MM-DD)"""
        original_count = len(series.dropna())

        # Try multiple date formats
        date_formats = [
            '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y', '%d-%m-%Y',
            '%Y/%m/%d', '%d.%m.%Y', '%Y.%m.%d'
        ]

        standardized = pd.to_datetime(series, errors='coerce', infer_datetime_format=True)
        standardized = standardized.dt.strftime('%Y-%m-%d')

        successful_count = len(standardized.dropna())
        return standardized, f"Standardized {successful_count}/{original_count} dates to ISO format"

    def _standardize_phones(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize phone numbers to international format"""
        def clean_phone(phone_str):
            if pd.isna(phone_str):
                return phone_str

            # Remove all non-numeric characters except +
            cleaned = re.sub(r'[^\d+]', '', str(phone_str))

            # Add country code if missing (assuming US for demo)
            if not cleaned.startswith('+'):
                if len(cleaned) == 10:
                    cleaned = '+1' + cleaned
                elif len(cleaned) == 11 and cleaned.startswith('1'):
                    cleaned = '+' + cleaned

            return cleaned

        standardized = series.apply(clean_phone)
        return standardized, f"Standardized phone numbers to international format"

    def _standardize_emails(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize email addresses"""
        def clean_email(email_str):
            if pd.isna(email_str):
                return email_str

            # Convert to lowercase and strip whitespace
            cleaned = str(email_str).lower().strip()

            # Basic email validation
            if re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', cleaned):
                return cleaned

            return None  # Invalid email

        standardized = series.apply(clean_email)
        return standardized, f"Standardized email addresses to lowercase"

    def _standardize_addresses(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize address formats"""
        def clean_address(addr_str):
            if pd.isna(addr_str):
                return addr_str

            # Title case and basic cleanup
            cleaned = str(addr_str).title().strip()

            # Common abbreviations
            abbreviations = {
                ' Street': ' St', ' Avenue': ' Ave', ' Boulevard': ' Blvd',
                ' Drive': ' Dr', ' Road': ' Rd', ' Lane': ' Ln'
            }

            for full, abbrev in abbreviations.items():
                cleaned = cleaned.replace(full, abbrev)

            return cleaned

        standardized = series.apply(clean_address)
        return standardized, f"Standardized address formats with common abbreviations"

    def _standardize_names(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize person names"""
        def clean_name(name_str):
            if pd.isna(name_str):
                return name_str

            # Title case and remove extra whitespace
            cleaned = ' '.join(str(name_str).title().split())
            return cleaned

        standardized = series.apply(clean_name)
        return standardized, f"Standardized names to title case"

    def _standardize_currency(self, series: pd.Series) -> Tuple[pd.Series, str]:
        """Standardize currency values"""
        def clean_currency(curr_str):
            if pd.isna(curr_str):
                return curr_str

            # Remove currency symbols and convert to float
            cleaned = re.sub(r'[^\d.-]', '', str(curr_str))
            try:
                return float(cleaned)
            except ValueError:
                return None

        standardized = series.apply(clean_currency)
        return standardized, f"Standardized currency to numeric format"

    # === VALUE VALIDATION ===

    def validate_values(
        self,
        df: pd.DataFrame,
        validation_rules: Dict[str, Dict[str, Any]]
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        Validate values against business rules

        Args:
            df: DataFrame to validate
            validation_rules: Dict with column names as keys, validation configs as values
                            Example: {
                                'iban': {'type': 'iban'},
                                'country': {'type': 'country_code'},
                                'postal_code': {'type': 'postal_code', 'country': 'US'}
                            }

        Returns:
            Tuple of (validated_df, validation_report)
        """
        validated_df = df.copy()
        report = {
            "columns_validated": [],
            "validation_results": {},
            "errors": {}
        }

        for column, validation_config in validation_rules.items():
            if column not in validated_df.columns:
                report["errors"][column] = "Column not found in dataset"
                continue

            try:
                validation_type = validation_config.get('type')

                if validation_type == 'iban':
                    result = self._validate_iban(validated_df[column])
                elif validation_type == 'country_code':
                    result = self._validate_country_codes(validated_df[column])
                elif validation_type == 'postal_code':
                    country = validation_config.get('country', 'US')
                    result = self._validate_postal_codes(validated_df[column], country)
                elif validation_type == 'length_range':
                    min_len = validation_config.get('min_length', 0)
                    max_len = validation_config.get('max_length', float('inf'))
                    result = self._validate_length_range(validated_df[column], min_len, max_len)
                elif validation_type == 'regex':
                    pattern = validation_config.get('pattern')
                    result = self._validate_regex_pattern(validated_df[column], pattern)
                else:
                    result = {"valid_count": 0, "invalid_count": 0, "message": f"Unknown validation type: {validation_type}"}

                report["columns_validated"].append(column)
                report["validation_results"][column] = result

            except Exception as e:
                report["errors"][column] = str(e)

        return validated_df, report

    def _validate_iban(self, series: pd.Series) -> Dict[str, Any]:
        """Validate IBAN format (simplified)"""
        def is_valid_iban(iban_str):
            if pd.isna(iban_str):
                return False

            # Basic IBAN validation (country code + 2 check digits + account identifier)
            iban = str(iban_str).replace(' ', '').upper()
            return len(iban) >= 15 and len(iban) <= 34 and iban[:2].isalpha() and iban[2:4].isdigit()

        valid_mask = series.apply(is_valid_iban)
        return {
            "valid_count": valid_mask.sum(),
            "invalid_count": (~valid_mask).sum(),
            "message": "IBAN format validation completed"
        }

    def _validate_country_codes(self, series: pd.Series) -> Dict[str, Any]:
        """Validate ISO country codes"""
        # Common ISO 3166-1 alpha-2 country codes (subset)
        valid_codes = {
            'US', 'CA', 'GB', 'DE', 'FR', 'IT', 'ES', 'NL', 'BE', 'AT',
            'CH', 'SE', 'NO', 'DK', 'FI', 'IE', 'PT', 'GR', 'PL', 'CZ',
            'AU', 'NZ', 'JP', 'KR', 'CN', 'IN', 'BR', 'MX', 'AR', 'CL'
        }

        def is_valid_country(country_str):
            if pd.isna(country_str):
                return False
            return str(country_str).upper() in valid_codes

        valid_mask = series.apply(is_valid_country)
        return {
            "valid_count": valid_mask.sum(),
            "invalid_count": (~valid_mask).sum(),
            "message": "Country code validation completed"
        }

    def _validate_postal_codes(self, series: pd.Series, country: str = 'US') -> Dict[str, Any]:
        """Validate postal codes for specific countries"""
        patterns = {
            'US': r'^\d{5}(-\d{4})?$',
            'CA': r'^[A-Za-z]\d[A-Za-z] \d[A-Za-z]\d$',
            'GB': r'^[A-Z]{1,2}[0-9R][0-9A-Z]? [0-9][A-BD-HJLNP-UW-Z]{2}$',
            'DE': r'^\d{5}$',
            'FR': r'^\d{5}$'
        }

        pattern = patterns.get(country.upper(), patterns['US'])

        def is_valid_postal(postal_str):
            if pd.isna(postal_str):
                return False
            return bool(re.match(pattern, str(postal_str).strip()))

        valid_mask = series.apply(is_valid_postal)
        return {
            "valid_count": valid_mask.sum(),
            "invalid_count": (~valid_mask).sum(),
            "message": f"Postal code validation for {country} completed"
        }

    def _validate_length_range(self, series: pd.Series, min_len: int, max_len: int) -> Dict[str, Any]:
        """Validate string length ranges"""
        def is_valid_length(value):
            if pd.isna(value):
                return False
            length = len(str(value))
            return min_len <= length <= max_len

        valid_mask = series.apply(is_valid_length)
        return {
            "valid_count": valid_mask.sum(),
            "invalid_count": (~valid_mask).sum(),
            "message": f"Length validation (min: {min_len}, max: {max_len}) completed"
        }

    def _validate_regex_pattern(self, series: pd.Series, pattern: str) -> Dict[str, Any]:
        """Validate values against regex pattern"""
        def matches_pattern(value):
            if pd.isna(value):
                return False
            return bool(re.match(pattern, str(value)))

        valid_mask = series.apply(matches_pattern)
        return {
            "valid_count": valid_mask.sum(),
            "invalid_count": (~valid_mask).sum(),
            "message": f"Regex pattern validation completed"
        }

    # === DATA CORRECTION WORKFLOWS ===

    def apply_corrections(
        self,
        dataset_id: str,
        corrections: List[Dict[str, Any]],
        user_id: str
    ) -> Dict[str, Any]:
        """
        Apply manual corrections to dataset

        Args:
            dataset_id: Dataset to correct
            corrections: List of correction dicts with row_index, column, new_value
            user_id: User applying corrections

        Returns:
            Correction report
        """
        # Load the current dataset
        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Dataset not found"
            )

        # Get the latest version
        latest_version = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .order_by(DatasetVersion.version_number.desc())
            .first()
        )

        if not latest_version:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="No dataset version found"
            )

        # Load the dataframe
        df = self.data_import_service.load_dataset_file(dataset_id, latest_version.version_number)

        corrections_applied = 0
        errors = []

        for correction in corrections:
            try:
                row_index = correction.get('row_index')
                column = correction.get('column')
                new_value = correction.get('new_value')
                issue_id = correction.get('issue_id')  # Optional reference to specific issue

                if row_index is not None and column in df.columns:
                    old_value = df.at[row_index, column]
                    df.at[row_index, column] = new_value
                    corrections_applied += 1

                    # Create fix record if issue_id is provided
                    if issue_id:
                        fix = Fix(
                            issue_id=issue_id,
                            fixed_by=user_id,
                            new_value=str(new_value),
                            comment=f"Manual correction: {old_value} -> {new_value}"
                        )
                        self.db.add(fix)

            except Exception as e:
                errors.append(f"Failed to apply correction {correction}: {str(e)}")

        # Save the corrected dataset as a new version
        if corrections_applied > 0:
            new_version_number = latest_version.version_number + 1
            file_path = self.data_import_service.save_dataset_file(
                dataset_id, df, new_version_number
            )

            # Create new dataset version record
            new_version = DatasetVersion(
                dataset_id=dataset_id,
                version_number=new_version_number,
                file_path=file_path,
                row_count=len(df),
                column_count=len(df.columns),
                notes=f"Applied {corrections_applied} manual corrections"
            )
            self.db.add(new_version)
            self.db.commit()

        return {
            "corrections_applied": corrections_applied,
            "errors": errors,
            "new_version_number": new_version_number if corrections_applied > 0 else None
        }

    def create_data_quality_summary(self, dataset_id: str) -> Dict[str, Any]:
        """Generate comprehensive data quality summary for a dataset"""

        # Get dataset and latest version
        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Dataset not found"
            )

        latest_version = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .order_by(DatasetVersion.version_number.desc())
            .first()
        )

        # Load the dataframe for analysis
        df = self.data_import_service.load_dataset_file(dataset_id, latest_version.version_number)

        # Get execution history and issues
        executions = (
            self.db.query(Execution)
            .filter(Execution.dataset_version_id == latest_version.id)
            .all()
        )

        total_issues = sum(exec.issues_found for exec in executions if exec.issues_found)
        total_fixes = (
            self.db.query(Fix)
            .join(Issue)
            .filter(Issue.execution_id.in_([e.id for e in executions]))
            .count()
        )

        # Basic data quality metrics
        quality_score = self._calculate_quality_score(df)

        return {
            "dataset_id": dataset_id,
            "dataset_name": dataset.name,
            "current_version": latest_version.version_number,
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "missing_data_percentage": (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
            "duplicate_rows": df.duplicated().sum(),
            "total_issues_found": total_issues,
            "total_fixes_applied": total_fixes,
            "data_quality_score": quality_score,
            "column_quality": self._analyze_column_quality(df),
            "execution_summary": {
                "total_executions": len(executions),
                "last_execution": executions[-1].created_at if executions else None,
                "success_rate": len([e for e in executions if e.status == "succeeded"]) / len(executions) * 100 if executions else 0
            }
        }

    def _calculate_quality_score(self, df: pd.DataFrame) -> float:
        """Calculate overall data quality score (0-100)"""
        factors = []

        # Completeness (no missing data)
        completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        factors.append(completeness)

        # Uniqueness (no duplicate rows)
        uniqueness = (1 - df.duplicated().sum() / len(df)) * 100 if len(df) > 0 else 100
        factors.append(uniqueness)

        # Consistency (uniform data types per column)
        consistency_scores = []
        for column in df.columns:
            non_null_values = df[column].dropna()
            if len(non_null_values) > 0:
                # Check if values are consistent with inferred type
                try:
                    if df[column].dtype == 'object':
                        # For object columns, check if they can be consistently parsed
                        pd.to_numeric(non_null_values, errors='raise')
                        consistency_scores.append(100)
                    else:
                        consistency_scores.append(100)
                except:
                    consistency_scores.append(90)  # Some inconsistency detected
            else:
                consistency_scores.append(0)

        consistency = np.mean(consistency_scores) if consistency_scores else 100
        factors.append(consistency)

        # Overall score is weighted average
        return np.mean(factors)

    def _analyze_column_quality(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """Analyze quality metrics for each column"""
        column_analysis = {}

        for column in df.columns:
            series = df[column]
            analysis = {
                "data_type": str(series.dtype),
                "missing_count": series.isnull().sum(),
                "missing_percentage": (series.isnull().sum() / len(series)) * 100,
                "unique_values": series.nunique(),
                "duplicate_count": len(series) - series.nunique(),
            }

            # Type-specific analysis
            if series.dtype in ['int64', 'float64']:
                analysis.update({
                    "min_value": series.min(),
                    "max_value": series.max(),
                    "mean_value": series.mean(),
                    "outliers_count": self._count_outliers(series)
                })
            elif series.dtype == 'object':
                analysis.update({
                    "avg_length": series.astype(str).str.len().mean(),
                    "min_length": series.astype(str).str.len().min(),
                    "max_length": series.astype(str).str.len().max(),
                })

            column_analysis[column] = analysis

        return column_analysis

    def _count_outliers(self, series: pd.Series) -> int:
        """Count outliers using IQR method"""
        try:
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = series[(series < lower_bound) | (series > upper_bound)]
            return len(outliers)
        except:
            return 0
</file>

<file path="api/app/services/export.py">
import pandas as pd
import json
import io
import zipfile
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
from sqlalchemy.orm import Session
from fastapi import HTTPException, status
from fastapi.responses import StreamingResponse

from app.models import (
    Dataset, DatasetVersion, Export, ExportFormat, Issue, Fix,
    Execution, User, DatasetStatus
)
from app.schemas import ExportCreate, ExportResponse
from app.services.data_import import DataImportService


class ExportService:
    """
    Service for exporting datasets and generating reports in multiple formats
    """

    def __init__(self, db: Session):
        self.db = db
        self.data_import_service = DataImportService(db)

        # Export storage directory
        self.export_storage_path = Path("data/exports")
        self.export_storage_path.mkdir(parents=True, exist_ok=True)

    # === CORE EXPORT FUNCTIONALITY ===

    def export_dataset(
        self,
        dataset_version_id: str,
        export_format: ExportFormat,
        user_id: str,
        execution_id: Optional[str] = None,
        include_metadata: bool = True,
        include_issues: bool = False
    ) -> Tuple[str, str]:
        """
        Export a dataset version in the specified format

        Args:
            dataset_version_id: ID of dataset version to export
            export_format: Format to export (csv, excel, json, etc.)
            user_id: ID of user requesting export
            execution_id: Optional execution ID for context
            include_metadata: Whether to include dataset metadata
            include_issues: Whether to include identified issues

        Returns:
            Tuple of (export_id, file_path)
        """
        # Get dataset version
        dataset_version = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.id == dataset_version_id)
            .first()
        )

        if not dataset_version:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Dataset version not found"
            )

        # Load the dataset
        df = self.data_import_service.load_dataset_file(
            dataset_version.dataset_id,
            dataset_version.version_number
        )

        # Generate export filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_version.dataset_id).first()
        base_filename = f"{dataset.name}_v{dataset_version.version_number}_{timestamp}"

        # Export based on format
        if export_format == ExportFormat.csv:
            file_path = self._export_csv(df, base_filename, include_metadata, dataset_version, include_issues)
        elif export_format == ExportFormat.excel:
            file_path = self._export_excel(df, base_filename, include_metadata, dataset_version, include_issues)
        elif export_format == ExportFormat.json:
            file_path = self._export_json(df, base_filename, include_metadata, dataset_version, include_issues)
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Export format {export_format.value} not supported"
            )

        # Create export record
        export_record = Export(
            dataset_version_id=dataset_version_id,
            execution_id=execution_id,
            format=export_format,
            location=str(file_path),
            created_by=user_id
        )

        self.db.add(export_record)
        self.db.commit()
        self.db.refresh(export_record)

        return export_record.id, str(file_path)

    def _export_csv(
        self,
        df: pd.DataFrame,
        base_filename: str,
        include_metadata: bool,
        dataset_version: DatasetVersion,
        include_issues: bool
    ) -> str:
        """Export dataset as CSV file(s)"""

        if not include_metadata and not include_issues:
            # Simple CSV export
            file_path = self.export_storage_path / f"{base_filename}.csv"
            df.to_csv(file_path, index=False)
            return str(file_path)

        # Create ZIP with multiple files
        zip_path = self.export_storage_path / f"{base_filename}.zip"

        with zipfile.ZipFile(zip_path, 'w') as zipf:
            # Main data file
            data_buffer = io.StringIO()
            df.to_csv(data_buffer, index=False)
            zipf.writestr(f"{base_filename}_data.csv", data_buffer.getvalue())

            # Metadata file
            if include_metadata:
                metadata = self._generate_metadata(dataset_version)
                zipf.writestr(f"{base_filename}_metadata.json", json.dumps(metadata, indent=2, default=str))

            # Issues file
            if include_issues:
                issues_df = self._get_issues_dataframe(dataset_version)
                if not issues_df.empty:
                    issues_buffer = io.StringIO()
                    issues_df.to_csv(issues_buffer, index=False)
                    zipf.writestr(f"{base_filename}_issues.csv", issues_buffer.getvalue())

        return str(zip_path)

    def _export_excel(
        self,
        df: pd.DataFrame,
        base_filename: str,
        include_metadata: bool,
        dataset_version: DatasetVersion,
        include_issues: bool
    ) -> str:
        """Export dataset as Excel file with multiple sheets"""

        file_path = self.export_storage_path / f"{base_filename}.xlsx"

        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # Main data sheet
            df.to_excel(writer, sheet_name='Data', index=False)

            # Metadata sheet
            if include_metadata:
                metadata = self._generate_metadata(dataset_version)
                metadata_df = pd.DataFrame([
                    {"Property": k, "Value": v} for k, v in metadata.items()
                ])
                metadata_df.to_excel(writer, sheet_name='Metadata', index=False)

            # Issues sheet
            if include_issues:
                issues_df = self._get_issues_dataframe(dataset_version)
                if not issues_df.empty:
                    issues_df.to_excel(writer, sheet_name='Issues', index=False)

            # Data quality summary sheet
            if include_metadata:
                quality_summary = self._generate_quality_summary(df)
                quality_df = pd.DataFrame([
                    {"Metric": k, "Value": v} for k, v in quality_summary.items()
                ])
                quality_df.to_excel(writer, sheet_name='Quality Summary', index=False)

        return str(file_path)

    def _export_json(
        self,
        df: pd.DataFrame,
        base_filename: str,
        include_metadata: bool,
        dataset_version: DatasetVersion,
        include_issues: bool
    ) -> str:
        """Export dataset as JSON file(s)"""

        export_data = {
            "data": df.to_dict(orient='records')
        }

        if include_metadata:
            export_data["metadata"] = self._generate_metadata(dataset_version)
            export_data["quality_summary"] = self._generate_quality_summary(df)

        if include_issues:
            issues_df = self._get_issues_dataframe(dataset_version)
            if not issues_df.empty:
                export_data["issues"] = issues_df.to_dict(orient='records')

        file_path = self.export_storage_path / f"{base_filename}.json"

        with open(file_path, 'w') as f:
            json.dump(export_data, f, indent=2, default=str)

        return str(file_path)

    # === METADATA AND QUALITY REPORTING ===

    def _generate_metadata(self, dataset_version: DatasetVersion) -> Dict[str, Any]:
        """Generate comprehensive metadata for dataset version"""

        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_version.dataset_id).first()

        # Get all versions for this dataset
        all_versions = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_version.dataset_id)
            .order_by(DatasetVersion.version_number.asc())
            .all()
        )

        # Get executions for this version
        executions = (
            self.db.query(Execution)
            .filter(Execution.dataset_version_id == dataset_version.id)
            .all()
        )

        return {
            "dataset_id": dataset.id,
            "dataset_name": dataset.name,
            "source_type": dataset.source_type.value,
            "original_filename": dataset.original_filename,
            "uploaded_by": dataset.uploaded_by,
            "uploaded_at": dataset.uploaded_at,
            "current_status": dataset.status.value,
            "version_info": {
                "version_number": dataset_version.version_number,
                "created_at": dataset_version.created_at,
                "row_count": dataset_version.row_count,
                "column_count": dataset_version.column_count,
                "notes": dataset_version.notes,
                "total_versions": len(all_versions)
            },
            "processing_history": {
                "total_executions": len(executions),
                "total_issues_found": sum(e.issues_found or 0 for e in executions),
                "last_execution": executions[-1].created_at if executions else None
            },
            "export_info": {
                "exported_at": datetime.now(),
                "export_version": "1.0"
            }
        }

    def _generate_quality_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """Generate data quality summary for the dataset"""

        total_cells = len(df) * len(df.columns)
        missing_cells = df.isnull().sum().sum()

        return {
            "total_rows": len(df),
            "total_columns": len(df.columns),
            "total_cells": total_cells,
            "missing_cells": missing_cells,
            "missing_percentage": (missing_cells / total_cells * 100) if total_cells > 0 else 0,
            "duplicate_rows": df.duplicated().sum(),
            "duplicate_percentage": (df.duplicated().sum() / len(df) * 100) if len(df) > 0 else 0,
            "data_types": df.dtypes.astype(str).to_dict(),
            "column_stats": {
                col: {
                    "missing_count": df[col].isnull().sum(),
                    "unique_values": df[col].nunique(),
                    "data_type": str(df[col].dtype)
                }
                for col in df.columns
            }
        }

    def _get_issues_dataframe(self, dataset_version: DatasetVersion) -> pd.DataFrame:
        """Get all issues for a dataset version as DataFrame"""

        # Get all executions for this dataset version
        executions = (
            self.db.query(Execution)
            .filter(Execution.dataset_version_id == dataset_version.id)
            .all()
        )

        if not executions:
            return pd.DataFrame()

        execution_ids = [e.id for e in executions]

        # Get all issues from these executions
        issues = (
            self.db.query(Issue)
            .filter(Issue.execution_id.in_(execution_ids))
            .all()
        )

        if not issues:
            return pd.DataFrame()

        # Convert to DataFrame
        issues_data = []
        for issue in issues:
            # Get fixes for this issue
            fixes = self.db.query(Fix).filter(Fix.issue_id == issue.id).all()

            issues_data.append({
                "issue_id": issue.id,
                "execution_id": issue.execution_id,
                "rule_id": issue.rule_id,
                "row_index": issue.row_index,
                "column_name": issue.column_name,
                "current_value": issue.current_value,
                "expected_value": issue.expected_value,
                "severity": issue.severity.value if issue.severity else None,
                "description": issue.description,
                "created_at": issue.created_at,
                "is_fixed": len(fixes) > 0,
                "fix_count": len(fixes),
                "latest_fix": fixes[-1].new_value if fixes else None,
                "latest_fix_at": fixes[-1].fixed_at if fixes else None
            })

        return pd.DataFrame(issues_data)

    # === EXPORT MANAGEMENT ===

    def get_export_history(self, dataset_id: str) -> List[Dict[str, Any]]:
        """Get export history for a dataset"""

        # Get all versions for this dataset
        versions = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .all()
        )

        version_ids = [v.id for v in versions]

        # Get all exports for these versions
        exports = (
            self.db.query(Export)
            .filter(Export.dataset_version_id.in_(version_ids))
            .order_by(Export.created_at.desc())
            .all()
        )

        export_history = []
        for export in exports:
            # Get creator info
            creator = self.db.query(User).filter(User.id == export.created_by).first()

            # Get version info
            version = next((v for v in versions if v.id == export.dataset_version_id), None)

            export_history.append({
                "export_id": export.id,
                "format": export.format.value,
                "created_at": export.created_at,
                "created_by": creator.name if creator else "Unknown",
                "dataset_version": version.version_number if version else None,
                "location": export.location,
                "execution_id": export.execution_id,
                "file_exists": Path(export.location).exists() if export.location else False
            })

        return export_history

    def get_export_file(self, export_id: str) -> Tuple[str, str]:
        """Get export file path and original filename"""

        export = self.db.query(Export).filter(Export.id == export_id).first()
        if not export:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Export not found"
            )

        if not export.location or not Path(export.location).exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Export file not found"
            )

        # Generate download filename
        dataset_version = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.id == export.dataset_version_id)
            .first()
        )

        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_version.dataset_id).first()

        download_filename = f"{dataset.name}_v{dataset_version.version_number}.{export.format.value}"

        return export.location, download_filename

    def delete_export(self, export_id: str, user_id: str) -> bool:
        """Delete an export and its associated file"""

        export = self.db.query(Export).filter(Export.id == export_id).first()
        if not export:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Export not found"
            )

        # Check permissions (only creator or admin can delete)
        user = self.db.query(User).filter(User.id == user_id).first()
        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="User not found"
            )

        if export.created_by != user_id and user.role.value != "admin":
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Insufficient permissions to delete this export"
            )

        # Delete file if it exists
        if export.location and Path(export.location).exists():
            try:
                Path(export.location).unlink()
            except Exception:
                pass  # Continue even if file deletion fails

        # Delete export record
        self.db.delete(export)
        self.db.commit()

        return True

    # === SPECIALIZED EXPORT FORMATS ===

    def export_data_quality_report(
        self,
        dataset_id: str,
        user_id: str,
        include_charts: bool = False
    ) -> Tuple[str, str]:
        """Generate comprehensive data quality report"""

        dataset = self.db.query(Dataset).filter(Dataset.id == dataset_id).first()
        if not dataset:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Dataset not found"
            )

        # Get latest version
        latest_version = (
            self.db.query(DatasetVersion)
            .filter(DatasetVersion.dataset_id == dataset_id)
            .order_by(DatasetVersion.version_number.desc())
            .first()
        )

        # Load dataset
        df = self.data_import_service.load_dataset_file(dataset_id, latest_version.version_number)

        # Generate comprehensive report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"{dataset.name}_quality_report_{timestamp}"

        file_path = self.export_storage_path / f"{report_filename}.xlsx"

        with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
            # Executive Summary
            summary_data = self._generate_executive_summary(dataset, latest_version, df)
            summary_df = pd.DataFrame([
                {"Metric": k, "Value": v} for k, v in summary_data.items()
            ])
            summary_df.to_excel(writer, sheet_name='Executive Summary', index=False)

            # Column Analysis
            column_analysis = self._generate_detailed_column_analysis(df)
            column_df = pd.DataFrame(column_analysis).T.reset_index()
            column_df.rename(columns={'index': 'Column'}, inplace=True)
            column_df.to_excel(writer, sheet_name='Column Analysis', index=False)

            # Issues Summary
            issues_df = self._get_issues_dataframe(latest_version)
            if not issues_df.empty:
                # Issues by severity
                severity_summary = issues_df.groupby('severity').size().reset_index(name='count')
                severity_summary.to_excel(writer, sheet_name='Issues by Severity', index=False)

                # Issues by column
                column_issues = issues_df.groupby('column_name').size().reset_index(name='issue_count')
                column_issues.to_excel(writer, sheet_name='Issues by Column', index=False)

                # All issues
                issues_df.to_excel(writer, sheet_name='All Issues', index=False)

            # Processing History
            executions = (
                self.db.query(Execution)
                .filter(Execution.dataset_version_id == latest_version.id)
                .all()
            )

            if executions:
                exec_data = []
                for exec in executions:
                    exec_data.append({
                        "Execution ID": exec.id,
                        "Created At": exec.created_at,
                        "Status": exec.status.value,
                        "Rules Executed": exec.rules_executed or 0,
                        "Issues Found": exec.issues_found or 0,
                        "Duration (seconds)": exec.duration_seconds or 0
                    })

                exec_df = pd.DataFrame(exec_data)
                exec_df.to_excel(writer, sheet_name='Processing History', index=False)

        # Create export record
        export_record = Export(
            dataset_version_id=latest_version.id,
            format=ExportFormat.excel,
            location=str(file_path),
            created_by=user_id
        )

        self.db.add(export_record)
        self.db.commit()
        self.db.refresh(export_record)

        return export_record.id, str(file_path)

    def _generate_executive_summary(
        self,
        dataset: Dataset,
        version: DatasetVersion,
        df: pd.DataFrame
    ) -> Dict[str, Any]:
        """Generate executive summary for quality report"""

        total_cells = len(df) * len(df.columns)
        missing_cells = df.isnull().sum().sum()

        # Calculate quality score
        completeness = (1 - missing_cells / total_cells) * 100 if total_cells > 0 else 100
        uniqueness = (1 - df.duplicated().sum() / len(df)) * 100 if len(df) > 0 else 100
        overall_quality = (completeness + uniqueness) / 2

        return {
            "Dataset Name": dataset.name,
            "Current Version": version.version_number,
            "Total Rows": len(df),
            "Total Columns": len(df.columns),
            "Total Data Points": total_cells,
            "Missing Data Points": missing_cells,
            "Missing Data %": round(missing_cells / total_cells * 100, 2) if total_cells > 0 else 0,
            "Duplicate Rows": df.duplicated().sum(),
            "Duplicate %": round(df.duplicated().sum() / len(df) * 100, 2) if len(df) > 0 else 0,
            "Data Completeness Score": round(completeness, 2),
            "Data Uniqueness Score": round(uniqueness, 2),
            "Overall Quality Score": round(overall_quality, 2),
            "Last Updated": version.created_at,
            "Report Generated": datetime.now()
        }

    def _generate_detailed_column_analysis(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """Generate detailed analysis for each column"""

        analysis = {}

        for column in df.columns:
            series = df[column]

            column_analysis = {
                "Data Type": str(series.dtype),
                "Total Values": len(series),
                "Missing Values": series.isnull().sum(),
                "Missing %": round(series.isnull().sum() / len(series) * 100, 2),
                "Unique Values": series.nunique(),
                "Duplicate Values": len(series) - series.nunique(),
                "Most Frequent Value": series.mode().iloc[0] if not series.mode().empty else None,
                "Value Frequency": series.value_counts().iloc[0] if not series.empty else 0
            }

            # Add type-specific analysis
            if series.dtype in ['int64', 'float64']:
                column_analysis.update({
                    "Min Value": series.min(),
                    "Max Value": series.max(),
                    "Mean": round(series.mean(), 2),
                    "Median": series.median(),
                    "Standard Deviation": round(series.std(), 2),
                    "Outliers (IQR)": self._count_outliers_iqr(series)
                })
            elif series.dtype == 'object':
                column_analysis.update({
                    "Min Length": series.astype(str).str.len().min(),
                    "Max Length": series.astype(str).str.len().max(),
                    "Avg Length": round(series.astype(str).str.len().mean(), 2),
                    "Empty Strings": (series == "").sum(),
                    "Whitespace Only": series.astype(str).str.strip().eq("").sum()
                })

            analysis[column] = column_analysis

        return analysis

    def _count_outliers_iqr(self, series: pd.Series) -> int:
        """Count outliers using IQR method"""
        try:
            Q1 = series.quantile(0.25)
            Q3 = series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = series[(series < lower_bound) | (series > upper_bound)]
            return len(outliers)
        except:
            return 0
</file>

<file path="api/app/services/rule_engine.py">
import pandas as pd
import json
import re
from typing import List, Dict, Any, Optional
from fastapi import HTTPException, status
from sqlalchemy.orm import Session
from abc import ABC, abstractmethod
from datetime import datetime, timezone

from app.models import (
    Rule, RuleKind, Execution, ExecutionRule, Issue, 
    DatasetVersion, User, Criticality, ExecutionStatus
)


class RuleValidator(ABC):
    """Abstract base class for all rule validators"""
    
    def __init__(self, rule: Rule, df: pd.DataFrame, db: Session):
        self.rule = rule
        self.df = df
        self.db = db
        # Handle SQLAlchemy model attribute access
        params_str = getattr(rule, 'params', None)
        self.params = json.loads(params_str) if params_str else {}
    
    @abstractmethod
    def validate(self) -> List[Dict[str, Any]]:
        """
        Validate data against the rule and return list of issues
        Returns: List of issue dictionaries with keys:
            - row_index: int
            - column_name: str  
            - current_value: str
            - suggested_value: str (optional)
            - message: str
            - category: str
        """
        pass


class MissingDataValidator(RuleValidator):
    """Validator for missing data detection"""
    
    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        
        if not target_columns:
            return issues
            
        for column in target_columns:
            if column not in self.df.columns:
                continue
                
            null_mask = self.df[column].isnull()
            null_indices = self.df[null_mask].index.tolist()
            
            for idx in null_indices:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': None,
                    'suggested_value': self.params.get('default_value', ''),
                    'message': f'Missing value in required field {column}',
                    'category': 'missing_data'
                })
        
        return issues


class StandardizationValidator(RuleValidator):
    """Validator for data standardization (dates, phones, emails, etc.)"""
    
    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        standardization_type = self.params.get('type', 'date')
        
        for column in target_columns:
            if column not in self.df.columns:
                continue
                
            if standardization_type == 'date':
                issues.extend(self._validate_dates(column))
            elif standardization_type == 'phone':
                issues.extend(self._validate_phones(column))
            elif standardization_type == 'email':
                issues.extend(self._validate_emails(column))
                
        return issues
    
    def _validate_dates(self, column: str) -> List[Dict[str, Any]]:
        issues = []
        date_format = self.params.get('format', '%Y-%m-%d')
        
        for idx, value in self.df[column].items():
            if pd.isnull(value):
                continue
                
            try:
                # Try to parse the date
                parsed_date = pd.to_datetime(value, format=date_format)
                # Check if it matches expected format
                if str(value) != parsed_date.strftime(date_format):
                    issues.append({
                        'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                        'column_name': column,
                        'current_value': str(value),
                        'suggested_value': parsed_date.strftime(date_format),
                        'message': f'Date format should be {date_format}',
                        'category': 'date_standardization'
                    })
            except:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': str(value),
                    'suggested_value': '',
                    'message': f'Invalid date format, expected {date_format}',
                    'category': 'date_standardization'
                })
        
        return issues
    
    def _validate_phones(self, column: str) -> List[Dict[str, Any]]:
        issues = []
        expected_format = self.params.get('format', '+1-XXX-XXX-XXXX')
        
        for idx, value in self.df[column].items():
            if pd.isnull(value):
                continue
                
            # Basic phone validation - customize based on requirements
            phone_str = str(value).strip()
            if not phone_str.startswith('+') or len(phone_str) < 10:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': str(value),
                    'suggested_value': f'+1-{phone_str}',
                    'message': f'Phone format should be {expected_format}',
                    'category': 'phone_standardization'
                })
        
        return issues
    
    def _validate_emails(self, column: str) -> List[Dict[str, Any]]:
        issues = []
        
        for idx, value in self.df[column].items():
            if pd.isnull(value):
                continue
                
            email_str = str(value).strip().lower()
            if '@' not in email_str or '.' not in email_str.split('@')[-1]:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': str(value),
                    'suggested_value': email_str,
                    'message': 'Invalid email format',
                    'category': 'email_standardization'
                })
        
        return issues


class ValueListValidator(RuleValidator):
    """Validator for allowed values list"""
    
    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        allowed_values = self.params.get('allowed_values', [])
        case_sensitive = self.params.get('case_sensitive', True)
        
        if not allowed_values:
            return issues
            
        for column in target_columns:
            if column not in self.df.columns:
                continue
                
            for idx, value in self.df[column].items():
                if pd.isnull(value):
                    continue
                    
                check_value = str(value) if case_sensitive else str(value).lower()
                check_allowed = allowed_values if case_sensitive else [v.lower() for v in allowed_values]
                
                if check_value not in check_allowed:
                    issues.append({
                        'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                        'column_name': column,
                        'current_value': str(value),
                        'suggested_value': allowed_values[0] if allowed_values else '',
                        'message': f'Value must be one of: {", ".join(allowed_values)}',
                        'category': 'value_list'
                    })
        
        return issues


class LengthRangeValidator(RuleValidator):
    """Validator for field length constraints"""
    
    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        min_length = self.params.get('min_length', 0)
        max_length = self.params.get('max_length', float('inf'))
        
        for column in target_columns:
            if column not in self.df.columns:
                continue
                
            for idx, value in self.df[column].items():
                if pd.isnull(value):
                    continue
                    
                value_length = len(str(value))
                
                if value_length < min_length:
                    issues.append({
                        'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                        'column_name': column,
                        'current_value': str(value),
                        'suggested_value': '',
                        'message': f'Value too short. Minimum length: {min_length}',
                        'category': 'length_range'
                    })
                elif value_length > max_length:
                    issues.append({
                        'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                        'column_name': column,
                        'current_value': str(value),
                        'suggested_value': str(value)[:max_length],
                        'message': f'Value too long. Maximum length: {max_length}',
                        'category': 'length_range'
                    })
        
        return issues


class CharRestrictionValidator(RuleValidator):
    """Validator for character restrictions (alphabetic only, etc.)"""
    
    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        restriction_type = self.params.get('type', 'alphabetic')
        
        for column in target_columns:
            if column not in self.df.columns:
                continue
                
            for idx, value in self.df[column].items():
                if pd.isnull(value):
                    continue
                    
                value_str = str(value)
                valid = True
                message = ''
                
                if restriction_type == 'alphabetic':
                    valid = value_str.replace(' ', '').isalpha()
                    message = 'Value must contain only alphabetic characters'
                elif restriction_type == 'numeric':
                    valid = value_str.replace('.', '').replace('-', '').isdigit()
                    message = 'Value must contain only numeric characters'
                elif restriction_type == 'alphanumeric':
                    valid = value_str.replace(' ', '').isalnum()
                    message = 'Value must contain only alphanumeric characters'
                
                if not valid:
                    issues.append({
                        'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                        'column_name': column,
                        'current_value': str(value),
                        'suggested_value': '',
                        'message': message,
                        'category': 'char_restriction'
                    })
        
        return issues


class CrossFieldValidator(RuleValidator):
    """Validator for cross-field relationships and dependencies"""

    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        rules = self.params.get('rules', [])

        if not rules:
            return issues

        for rule_def in rules:
            rule_type = rule_def.get('type', '')

            if rule_type == 'dependency':
                # Field A depends on Field B (if B has value, A must have value)
                issues.extend(self._validate_dependency(rule_def))
            elif rule_type == 'mutual_exclusion':
                # Fields A and B cannot both have values
                issues.extend(self._validate_mutual_exclusion(rule_def))
            elif rule_type == 'conditional':
                # If field A has specific value, field B must have specific value
                issues.extend(self._validate_conditional(rule_def))
            elif rule_type == 'sum_check':
                # Sum of multiple fields must equal specific value or field
                issues.extend(self._validate_sum_check(rule_def))

        return issues

    def _validate_dependency(self, rule_def: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Validate field dependency rules"""
        issues = []
        dependent_field = rule_def.get('dependent_field')
        required_field = rule_def.get('required_field')

        if not dependent_field or not required_field:
            return issues

        if dependent_field not in self.df.columns or required_field not in self.df.columns:
            return issues

        for idx, row in self.df.iterrows():
            required_value = row[required_field]
            dependent_value = row[dependent_field]

            # If required field has value but dependent field doesn't
            if pd.notna(required_value) and pd.isna(dependent_value):
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': dependent_field,
                    'current_value': None,
                    'suggested_value': '',
                    'message': f'{dependent_field} is required when {required_field} has a value',
                    'category': 'cross_field_dependency'
                })

        return issues

    def _validate_mutual_exclusion(self, rule_def: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Validate mutual exclusion rules"""
        issues = []
        fields = rule_def.get('fields', [])

        if len(fields) < 2:
            return issues

        available_fields = [f for f in fields if f in self.df.columns]
        if len(available_fields) < 2:
            return issues

        for idx, row in self.df.iterrows():
            filled_fields = [f for f in available_fields if pd.notna(row[f])]

            if len(filled_fields) > 1:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': ', '.join(filled_fields),
                    'current_value': f"Multiple fields filled: {', '.join(filled_fields)}",
                    'suggested_value': 'Only one field should have a value',
                    'message': f'Fields {", ".join(available_fields)} are mutually exclusive',
                    'category': 'cross_field_mutual_exclusion'
                })

        return issues

    def _validate_conditional(self, rule_def: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Validate conditional field rules"""
        issues = []
        condition_field = rule_def.get('condition_field')
        condition_value = rule_def.get('condition_value')
        target_field = rule_def.get('target_field')
        expected_value = rule_def.get('expected_value')

        if not all([condition_field, target_field]):
            return issues

        if condition_field not in self.df.columns or target_field not in self.df.columns:
            return issues

        for idx, row in self.df.iterrows():
            if str(row[condition_field]) == str(condition_value):
                target_value = row[target_field]

                if expected_value is not None:
                    # Check for specific expected value
                    if str(target_value) != str(expected_value):
                        issues.append({
                            'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                            'column_name': target_field,
                            'current_value': str(target_value),
                            'suggested_value': str(expected_value),
                            'message': f'When {condition_field} is {condition_value}, {target_field} must be {expected_value}',
                            'category': 'cross_field_conditional'
                        })
                else:
                    # Check for any value (not null)
                    if pd.isna(target_value):
                        issues.append({
                            'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                            'column_name': target_field,
                            'current_value': None,
                            'suggested_value': '',
                            'message': f'When {condition_field} is {condition_value}, {target_field} must have a value',
                            'category': 'cross_field_conditional'
                        })

        return issues

    def _validate_sum_check(self, rule_def: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Validate sum check rules"""
        issues = []
        sum_fields = rule_def.get('sum_fields', [])
        total_field = rule_def.get('total_field')
        expected_total = rule_def.get('expected_total')

        if not sum_fields:
            return issues

        available_sum_fields = [f for f in sum_fields if f in self.df.columns]
        if not available_sum_fields:
            return issues

        for idx, row in self.df.iterrows():
            # Calculate sum of numeric fields only
            field_sum = 0
            for field in available_sum_fields:
                try:
                    value = pd.to_numeric(row[field], errors='coerce')
                    if pd.notna(value):
                        field_sum += value
                except:
                    continue

            # Check against total field or expected value
            if total_field and total_field in self.df.columns:
                try:
                    expected = pd.to_numeric(row[total_field], errors='coerce')
                    if pd.notna(expected) and abs(field_sum - expected) > 0.01:  # Allow small floating point differences
                        issues.append({
                            'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                            'column_name': total_field,
                            'current_value': str(row[total_field]),
                            'suggested_value': str(field_sum),
                            'message': f'Sum of {", ".join(available_sum_fields)} ({field_sum}) does not match {total_field}',
                            'category': 'cross_field_sum_check'
                        })
                except:
                    continue
            elif expected_total is not None:
                try:
                    expected = float(expected_total)
                    if abs(field_sum - expected) > 0.01:
                        issues.append({
                            'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                            'column_name': ', '.join(available_sum_fields),
                            'current_value': str(field_sum),
                            'suggested_value': str(expected_total),
                            'message': f'Sum of {", ".join(available_sum_fields)} ({field_sum}) does not equal expected total ({expected_total})',
                            'category': 'cross_field_sum_check'
                        })
                except:
                    continue

        return issues


class RegexValidator(RuleValidator):
    """Validator for regular expression pattern matching"""

    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        target_columns = self.params.get('columns', [])
        patterns = self.params.get('patterns', [])

        if not patterns:
            return issues

        for column in target_columns:
            if column not in self.df.columns:
                continue

            for pattern_def in patterns:
                pattern = pattern_def.get('pattern')
                pattern_name = pattern_def.get('name', 'pattern')
                must_match = pattern_def.get('must_match', True)

                if not pattern:
                    continue

                try:
                    compiled_pattern = re.compile(pattern)
                    issues.extend(self._validate_pattern(
                        column, compiled_pattern, pattern_name, must_match, pattern
                    ))
                except re.error as e:
                    # Invalid regex pattern
                    continue

        return issues

    def _validate_pattern(
        self,
        column: str,
        compiled_pattern: re.Pattern,
        pattern_name: str,
        must_match: bool,
        original_pattern: str
    ) -> List[Dict[str, Any]]:
        """Validate a specific regex pattern against a column"""
        issues = []

        for idx, value in self.df[column].items():
            if pd.isna(value):
                continue

            value_str = str(value)
            matches = bool(compiled_pattern.search(value_str))

            if must_match and not matches:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': value_str,
                    'suggested_value': '',
                    'message': f'Value does not match required pattern "{pattern_name}" ({original_pattern})',
                    'category': 'regex_validation'
                })
            elif not must_match and matches:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': column,
                    'current_value': value_str,
                    'suggested_value': '',
                    'message': f'Value matches forbidden pattern "{pattern_name}" ({original_pattern})',
                    'category': 'regex_validation'
                })

        return issues


class CustomValidator(RuleValidator):
    """Validator for custom user-defined validation logic"""

    def validate(self) -> List[Dict[str, Any]]:
        issues = []
        validation_type = self.params.get('type', 'python_expression')

        if validation_type == 'python_expression':
            issues.extend(self._validate_python_expression())
        elif validation_type == 'lookup_table':
            issues.extend(self._validate_lookup_table())
        elif validation_type == 'custom_function':
            issues.extend(self._validate_custom_function())

        return issues

    def _validate_python_expression(self) -> List[Dict[str, Any]]:
        """Validate using Python expressions (be careful with security!)"""
        issues = []
        expression = self.params.get('expression', '')
        target_columns = self.params.get('columns', [])
        error_message = self.params.get('error_message', 'Custom validation failed')

        if not expression or not target_columns:
            return issues

        # Security: Only allow basic operations and pandas/numpy functions
        allowed_names = {
            'pd': pd, 'abs': abs, 'len': len, 'str': str, 'int': int, 'float': float,
            'min': min, 'max': max, 'sum': sum, 'round': round
        }

        for idx, row in self.df.iterrows():
            try:
                # Create context with row data and safe functions
                context = allowed_names.copy()
                context.update({col: row[col] for col in self.df.columns})
                context['row'] = row

                # Evaluate expression safely
                result = eval(expression, {"__builtins__": {}}, context)

                # If result is False, it's an issue
                if not result:
                    for column in target_columns:
                        if column in self.df.columns:
                            issues.append({
                                'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                                'column_name': column,
                                'current_value': str(row[column]) if pd.notna(row[column]) else None,
                                'suggested_value': '',
                                'message': error_message,
                                'category': 'custom_validation'
                            })
                            break  # Only add issue once per row

            except Exception as e:
                # Skip rows where expression fails
                continue

        return issues

    def _validate_lookup_table(self) -> List[Dict[str, Any]]:
        """Validate using lookup table mappings"""
        issues = []
        lookup_table = self.params.get('lookup_table', {})
        lookup_column = self.params.get('lookup_column')
        target_column = self.params.get('target_column')

        if not lookup_table or not lookup_column or not target_column:
            return issues

        if lookup_column not in self.df.columns or target_column not in self.df.columns:
            return issues

        for idx, row in self.df.iterrows():
            lookup_value = str(row[lookup_column]) if pd.notna(row[lookup_column]) else ''
            target_value = str(row[target_column]) if pd.notna(row[target_column]) else ''
            expected_value = lookup_table.get(lookup_value, '')

            if expected_value and target_value != expected_value:
                issues.append({
                    'row_index': int(str(idx)) if not isinstance(idx, int) else idx,
                    'column_name': target_column,
                    'current_value': target_value,
                    'suggested_value': expected_value,
                    'message': f'Based on {lookup_column} value "{lookup_value}", {target_column} should be "{expected_value}"',
                    'category': 'custom_lookup'
                })

        return issues

    def _validate_custom_function(self) -> List[Dict[str, Any]]:
        """Validate using custom function logic"""
        issues = []
        # This would need to be implemented based on specific requirements
        # For now, return empty list as placeholder
        function_name = self.params.get('function_name', '')

        # In a real implementation, you might:
        # 1. Load a function from a registry
        # 2. Execute a stored procedure
        # 3. Call an external API

        return issues


class RuleEngineService:
    """Main service for rule engine operations"""
    
    def __init__(self, db: Session):
        self.db = db
        self.validators = {
            RuleKind.missing_data: MissingDataValidator,
            RuleKind.standardization: StandardizationValidator,
            RuleKind.value_list: ValueListValidator,
            RuleKind.length_range: LengthRangeValidator,
            RuleKind.char_restriction: CharRestrictionValidator,
            RuleKind.cross_field: CrossFieldValidator,
            RuleKind.regex: RegexValidator,
            RuleKind.custom: CustomValidator,
        }
    
    def get_active_rules(self) -> List[Rule]:
        """Get all active rules"""
        return self.db.query(Rule).filter(Rule.is_active == True).all()
    
    def get_rule_by_id(self, rule_id: str) -> Optional[Rule]:
        """Get rule by ID"""
        return self.db.query(Rule).filter(Rule.id == rule_id).first()
    
    def create_rule(
        self, 
        name: str,
        description: str,
        kind: RuleKind,
        criticality: Criticality,
        target_columns: List[str],
        params: Dict[str, Any],
        current_user: User
    ) -> Rule:
        """Create a new business rule"""
        
        # Check if rule name already exists
        existing_rule = self.db.query(Rule).filter(Rule.name == name).first()
        if existing_rule:
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"Rule with name '{name}' already exists"
            )
        
        rule = Rule(
            name=name,
            description=description,
            kind=kind,
            criticality=criticality,
            target_columns=json.dumps(target_columns),
            params=json.dumps(params),
            created_by=current_user.id,
            is_active=True
        )
        
        self.db.add(rule)
        self.db.commit()
        self.db.refresh(rule)
        
        return rule
    
    def execute_rules_on_dataset(
        self, 
        dataset_version: DatasetVersion, 
        rule_ids: Optional[List[str]], 
        current_user: User
    ) -> Execution:
        """Execute rules on a dataset version"""
        
        # Get rules to execute
        if rule_ids:
            rules = self.db.query(Rule).filter(
                Rule.id.in_(rule_ids),
                Rule.is_active == True
            ).all()
        else:
            rules = self.get_active_rules()
        
        if not rules:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="No active rules found to execute"
            )
        
        # Create execution record
        execution = Execution(
            dataset_version_id=dataset_version.id,
            started_by=current_user.id,
            status=ExecutionStatus.running,
            total_rules=len(rules)
        )
        
        self.db.add(execution)
        self.db.commit()
        self.db.refresh(execution)
        
        try:
            # Load dataset data (this would need to be implemented based on how data is stored)
            # For now, assuming we have a method to load the dataset as DataFrame
            df = self._load_dataset_as_dataframe(dataset_version)
            
            execution.total_rows = len(df)
            all_issues = []
            
            # Execute each rule
            execution_rule = None
            for rule in rules:
                try:
                    execution_rule = ExecutionRule(
                        execution_id=execution.id,
                        rule_id=rule.id
                    )
                    self.db.add(execution_rule)
                    
                    # Get validator for this rule type
                    rule_kind = getattr(rule, 'kind', None)
                    if rule_kind is None:
                        execution_rule.note = "Rule has no kind specified"
                        continue
                    validator_class = self.validators.get(rule_kind)
                    if not validator_class:
                        execution_rule.note = f"No validator available for rule kind: {rule_kind}"
                        continue
                    
                    # Run validation
                    validator = validator_class(rule, df, self.db)
                    issues = validator.validate()
                    
                    # Create issue records
                    for issue_data in issues:
                        issue = Issue(
                            execution_id=execution.id,
                            rule_id=rule.id,
                            row_index=issue_data['row_index'],
                            column_name=issue_data['column_name'],
                            current_value=issue_data['current_value'],
                            suggested_value=issue_data.get('suggested_value'),
                            message=issue_data['message'],
                            category=issue_data['category'],
                            severity=rule.criticality
                        )
                        self.db.add(issue)
                        all_issues.append(issue)
                    
                    # Update execution rule stats
                    execution_rule.error_count = len(issues)
                    execution_rule.rows_flagged = len(set(i['row_index'] for i in issues))
                    execution_rule.cols_flagged = len(set(i['column_name'] for i in issues))
                    
                except Exception as e:
                    execution_rule.note = f"Error executing rule: {str(e)}"
            
            # Update execution summary
            execution.status = ExecutionStatus.succeeded
            execution.finished_at = datetime.now(timezone.utc)
            execution.rows_affected = len(set(issue.row_index for issue in all_issues))
            execution.columns_affected = len(set(issue.column_name for issue in all_issues))
            execution.summary = json.dumps({
                'total_issues': len(all_issues),
                'issues_by_severity': self._count_issues_by_severity(all_issues),
                'issues_by_category': self._count_issues_by_category(all_issues)
            })
            
            self.db.commit()
            
        except Exception as e:
            execution.status = ExecutionStatus.failed
            execution.finished_at = datetime.now(timezone.utc)
            execution.summary = json.dumps({'error': str(e)})
            self.db.commit()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Rule execution failed: {str(e)}"
            )
        
        return execution
    
    def _load_dataset_as_dataframe(self, dataset_version) -> pd.DataFrame:
        """Load dataset version as pandas DataFrame"""
        from app.services.data_import import DataImportService

        try:
            # Use the data import service to load the dataset file
            data_service = DataImportService(self.db)
            df = data_service.load_dataset_file(
                dataset_version.dataset_id,
                dataset_version.version_no
            )
            return df
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to load dataset: {str(e)}"
            )
    
    def _count_issues_by_severity(self, issues: List[Issue]) -> Dict[str, int]:
        """Count issues by severity level"""
        counts = {}
        for issue in issues:
            severity = issue.severity.value if hasattr(issue.severity, 'value') else str(issue.severity)
            counts[severity] = counts.get(severity, 0) + 1
        return counts
    
    def _count_issues_by_category(self, issues: List[Issue]) -> Dict[str, int]:
        """Count issues by category"""
        counts = {}
        for issue in issues:
            category = issue.category or 'unknown'
            counts[category] = counts.get(category, 0) + 1
        return counts
</file>

<file path="api/app/auth.py">
from datetime import datetime, timedelta, timezone
from typing import Optional
from fastapi import HTTPException, status, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from passlib.context import CryptContext
from jose import JWTError, jwt
import os
from sqlalchemy.orm import Session
from app.models import User, UserRole
from app.database import get_session

SECRET_KEY = os.getenv("SECRET_KEY", "your-secret-key-here")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

security = HTTPBearer()
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

def verify_token(token: str) -> Optional[dict]:
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    db: Session = Depends(get_session)
) -> User:
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    token = credentials.credentials
    payload = verify_token(token)
    if payload is None:
        raise credentials_exception
    
    user_id: str = payload.get("sub")
    if user_id is None:
        raise credentials_exception
    
    user = db.query(User).filter(User.id == user_id).first()
    if user is None:
        raise credentials_exception
    
    return user

def require_role(allowed_roles: list[UserRole]):
    def role_checker(current_user: User = Depends(get_current_user)) -> User:
        if current_user.role not in allowed_roles:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Insufficient permissions"
            )
        return current_user
    return role_checker

# Role-specific dependencies
def get_admin_user(current_user: User = Depends(require_role([UserRole.admin]))) -> User:
    return current_user

def get_admin_or_analyst_user(
    current_user: User = Depends(require_role([UserRole.admin, UserRole.analyst]))
) -> User:
    return current_user

def get_any_authenticated_user(current_user: User = Depends(get_current_user)) -> User:
    return current_user
</file>

<file path="api/app/database.py">
from sqlalchemy import create_engine, MetaData
from sqlalchemy.orm import sessionmaker, declarative_base
import os
from dotenv import load_dotenv


# Import Database URL
dotenv_loaded = load_dotenv(
    dotenv_path="../migrations/.env.local", override=True)
DB_URL = os.getenv("DATABASE_URL")
if DB_URL is None:
    raise ValueError("DATABASE_URL environment variable is not set.")

engine = create_engine(DB_URL, pool_pre_ping=True)

SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)
Base = declarative_base()
metadata = Base.metadata


def get_session():
    with SessionLocal() as session:
        yield session
</file>

<file path="api/app/main.py">
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.routes.upload import upload
from app.routes.auth import auth
from app.routes import rules, executions, processing, reports

app = FastAPI(
    title="Data Hygiene Tool API",
    description="API for data quality management and cleansing",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure this for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(auth.router)
app.include_router(upload.router)
app.include_router(rules.router)
app.include_router(executions.router)
app.include_router(processing.router)
app.include_router(reports.router)

@app.get("/")
def read_root():
    return {"message": "Data Hygiene Tool API", "version": "1.0.0"}
</file>

<file path="api/app/models.py">
from sqlalchemy import Column, String, Integer, DateTime, Boolean, Text, ForeignKey, func
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import UUID, ENUM
from app.database import Base
import uuid
import enum

# Enums
class UserRole(enum.Enum):
    admin = "admin"
    analyst = "analyst"
    viewer = "viewer"

class SourceType(enum.Enum):
    csv = "csv"
    excel = "excel"
    sap = "sap"
    ms_dynamics = "ms_dynamics"
    other = "other"

class DatasetStatus(enum.Enum):
    uploaded = "uploaded"
    profiled = "profiled"
    validated = "validated"
    cleaned = "cleaned"
    exported = "exported"

class Criticality(enum.Enum):
    low = "low"
    medium = "medium"
    high = "high"
    critical = "critical"

class RuleKind(enum.Enum):
    missing_data = "missing_data"
    standardization = "standardization"
    value_list = "value_list"
    length_range = "length_range"
    cross_field = "cross_field"
    char_restriction = "char_restriction"
    regex = "regex"
    custom = "custom"

class ExecutionStatus(enum.Enum):
    queued = "queued"
    running = "running"
    succeeded = "succeeded"
    failed = "failed"
    partially_succeeded = "partially_succeeded"

class ExportFormat(enum.Enum):
    csv = "csv"
    excel = "excel"
    json = "json"
    api = "api"
    datalake = "datalake"

# Models
class User(Base):
    __tablename__ = "users"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    name = Column(String, nullable=False)
    email = Column(String, unique=True, nullable=False)
    role = Column(ENUM(UserRole), nullable=False)
    auth_provider = Column(String)
    auth_subject = Column(String)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Relationships
    uploaded_datasets = relationship("Dataset", back_populates="uploader")
    created_rules = relationship("Rule", back_populates="creator")
    started_executions = relationship("Execution", back_populates="starter")
    fixed_issues = relationship("Fix", back_populates="fixer")
    created_exports = relationship("Export", back_populates="creator")

class Dataset(Base):
    __tablename__ = "datasets"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    name = Column(String, nullable=False)
    source_type = Column(ENUM(SourceType), nullable=False)
    original_filename = Column(String)
    checksum = Column(String)
    uploaded_by = Column(String, ForeignKey("users.id"), nullable=False)
    uploaded_at = Column(DateTime, server_default=func.now())
    status = Column(ENUM(DatasetStatus), default=DatasetStatus.uploaded)
    row_count = Column(Integer)
    column_count = Column(Integer)
    notes = Column(Text)
    
    # Relationships
    uploader = relationship("User", back_populates="uploaded_datasets")
    versions = relationship("DatasetVersion", back_populates="dataset")
    columns = relationship("DatasetColumn", back_populates="dataset")

class DatasetVersion(Base):
    __tablename__ = "dataset_versions"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    dataset_id = Column(String, ForeignKey("datasets.id"), nullable=False)
    version_no = Column(Integer, nullable=False)
    created_by = Column(String, ForeignKey("users.id"), nullable=False)
    created_at = Column(DateTime, server_default=func.now())
    rows = Column(Integer)
    columns = Column(Integer)
    change_note = Column(Text)
    
    # Relationships
    dataset = relationship("Dataset", back_populates="versions")
    creator = relationship("User")
    executions = relationship("Execution", back_populates="dataset_version")
    exports = relationship("Export", back_populates="dataset_version")
    journal_entries = relationship("VersionJournal", back_populates="dataset_version")

class DatasetColumn(Base):
    __tablename__ = "dataset_columns"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    dataset_id = Column(String, ForeignKey("datasets.id"), nullable=False)
    name = Column(String, nullable=False)
    ordinal_position = Column(Integer, nullable=False)
    inferred_type = Column(String)
    is_nullable = Column(Boolean, default=True)
    
    # Relationships
    dataset = relationship("Dataset", back_populates="columns")
    rule_columns = relationship("RuleColumn", back_populates="column")

class Rule(Base):
    __tablename__ = "rules"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    name = Column(String, unique=True, nullable=False)
    description = Column(Text)
    kind = Column(ENUM(RuleKind), nullable=False)
    criticality = Column(ENUM(Criticality), nullable=False)
    is_active = Column(Boolean, default=True)
    target_table = Column(String)
    target_columns = Column(Text)
    params = Column(Text)  # JSON as text
    created_by = Column(String, ForeignKey("users.id"), nullable=False)
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
    
    # Relationships
    creator = relationship("User", back_populates="created_rules")
    rule_columns = relationship("RuleColumn", back_populates="rule")
    execution_rules = relationship("ExecutionRule", back_populates="rule")
    issues = relationship("Issue", back_populates="rule")

class RuleColumn(Base):
    __tablename__ = "rule_columns"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    rule_id = Column(String, ForeignKey("rules.id"), nullable=False)
    column_id = Column(String, ForeignKey("dataset_columns.id"), nullable=False)
    
    # Relationships
    rule = relationship("Rule", back_populates="rule_columns")
    column = relationship("DatasetColumn", back_populates="rule_columns")

class Execution(Base):
    __tablename__ = "executions"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    dataset_version_id = Column(String, ForeignKey("dataset_versions.id"), nullable=False)
    started_by = Column(String, ForeignKey("users.id"), nullable=False)
    started_at = Column(DateTime, server_default=func.now())
    finished_at = Column(DateTime)
    status = Column(ENUM(ExecutionStatus), default=ExecutionStatus.queued)
    total_rows = Column(Integer)
    total_rules = Column(Integer)
    rows_affected = Column(Integer)
    columns_affected = Column(Integer)
    summary = Column(Text)  # JSON as text
    
    # Relationships
    dataset_version = relationship("DatasetVersion", back_populates="executions")
    starter = relationship("User", back_populates="started_executions")
    execution_rules = relationship("ExecutionRule", back_populates="execution")
    issues = relationship("Issue", back_populates="execution")
    exports = relationship("Export", back_populates="execution")

class ExecutionRule(Base):
    __tablename__ = "execution_rules"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    execution_id = Column(String, ForeignKey("executions.id"), nullable=False)
    rule_id = Column(String, ForeignKey("rules.id"), nullable=False)
    error_count = Column(Integer, default=0)
    rows_flagged = Column(Integer, default=0)
    cols_flagged = Column(Integer, default=0)
    note = Column(Text)
    
    # Relationships
    execution = relationship("Execution", back_populates="execution_rules")
    rule = relationship("Rule", back_populates="execution_rules")

class Issue(Base):
    __tablename__ = "issues"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    execution_id = Column(String, ForeignKey("executions.id"), nullable=False)
    rule_id = Column(String, ForeignKey("rules.id"), nullable=False)
    row_index = Column(Integer, nullable=False)
    column_name = Column(String, nullable=False)
    current_value = Column(Text)
    suggested_value = Column(Text)
    message = Column(Text)
    category = Column(String)
    severity = Column(ENUM(Criticality), nullable=False)
    created_at = Column(DateTime, server_default=func.now())
    resolved = Column(Boolean, default=False)
    
    # Relationships
    execution = relationship("Execution", back_populates="issues")
    rule = relationship("Rule", back_populates="issues")
    fixes = relationship("Fix", back_populates="issue")

class Fix(Base):
    __tablename__ = "fixes"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    issue_id = Column(String, ForeignKey("issues.id"), nullable=False)
    fixed_by = Column(String, ForeignKey("users.id"), nullable=False)
    fixed_at = Column(DateTime, server_default=func.now())
    new_value = Column(Text)
    comment = Column(Text)
    
    # Relationships
    issue = relationship("Issue", back_populates="fixes")
    fixer = relationship("User", back_populates="fixed_issues")

class Export(Base):
    __tablename__ = "exports"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    dataset_version_id = Column(String, ForeignKey("dataset_versions.id"), nullable=False)
    execution_id = Column(String, ForeignKey("executions.id"))
    format = Column(ENUM(ExportFormat), nullable=False)
    location = Column(String)
    created_by = Column(String, ForeignKey("users.id"), nullable=False)
    created_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    dataset_version = relationship("DatasetVersion", back_populates="exports")
    execution = relationship("Execution", back_populates="exports")
    creator = relationship("User", back_populates="created_exports")

class VersionJournal(Base):
    __tablename__ = "version_journal"
    
    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()), index=True)
    dataset_version_id = Column(String, ForeignKey("dataset_versions.id"), nullable=False)
    event = Column(String, nullable=False)
    rows_affected = Column(Integer)
    columns_affected = Column(Integer)
    details = Column(Text)
    occurred_at = Column(DateTime, server_default=func.now())
    
    # Relationships
    dataset_version = relationship("DatasetVersion", back_populates="journal_entries")
</file>

<file path="api/app/schemas.py">
from pydantic import BaseModel, EmailStr, ConfigDict
from typing import Optional, List
from datetime import datetime
from app.models import (
    UserRole, SourceType, DatasetStatus, Criticality, RuleKind, 
    ExecutionStatus, ExportFormat
)

# Base schemas
class BaseSchema(BaseModel):
    model_config = ConfigDict(from_attributes=True)

# User schemas
class UserBase(BaseModel):
    name: str
    email: EmailStr
    role: UserRole

class UserCreate(UserBase):
    password: str

class UserLogin(BaseModel):
    email: EmailStr
    password: str

class UserResponse(UserBase):
    id: str
    created_at: datetime
    updated_at: datetime
    model_config = ConfigDict(from_attributes=True)

class TokenResponse(BaseModel):
    access_token: str
    token_type: str
    user: UserResponse

# Dataset schemas
class DatasetBase(BaseModel):
    name: str
    source_type: SourceType
    notes: Optional[str] = None

class DatasetCreate(DatasetBase):
    original_filename: Optional[str] = None

class DatasetResponse(DatasetBase):
    id: str
    original_filename: Optional[str]
    checksum: Optional[str]
    uploaded_by: str
    uploaded_at: datetime
    status: DatasetStatus
    row_count: Optional[int]
    column_count: Optional[int]
    model_config = ConfigDict(from_attributes=True)

class DatasetVersionBase(BaseModel):
    version_no: int
    change_note: Optional[str] = None

class DatasetVersionCreate(DatasetVersionBase):
    pass

class DatasetVersionResponse(DatasetVersionBase):
    id: str
    dataset_id: str
    created_by: str
    created_at: datetime
    rows: Optional[int]
    columns: Optional[int]
    model_config = ConfigDict(from_attributes=True)

# Dataset Column schemas
class DatasetColumnBase(BaseModel):
    name: str
    ordinal_position: int
    inferred_type: Optional[str] = None
    is_nullable: bool = True

class DatasetColumnCreate(DatasetColumnBase):
    pass

class DatasetColumnResponse(DatasetColumnBase):
    id: str
    dataset_id: str
    model_config = ConfigDict(from_attributes=True)

# Rule schemas
class RuleBase(BaseModel):
    name: str
    description: Optional[str] = None
    kind: RuleKind
    criticality: Criticality
    is_active: bool = True
    target_table: Optional[str] = None
    target_columns: Optional[str] = None
    params: Optional[str] = None  # JSON string

class RuleCreate(BaseModel):
    name: str
    description: Optional[str] = None
    kind: RuleKind
    criticality: Criticality
    target_columns: List[str]
    params: dict = {}

class RuleUpdate(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    kind: Optional[RuleKind] = None
    criticality: Optional[Criticality] = None
    is_active: Optional[bool] = None
    target_table: Optional[str] = None
    target_columns: Optional[List[str]] = None
    params: Optional[dict] = None

class RuleResponse(RuleBase):
    id: str
    created_by: str
    created_at: datetime
    updated_at: datetime
    model_config = ConfigDict(from_attributes=True)

# Execution schemas
class ExecutionBase(BaseModel):
    dataset_version_id: str

class ExecutionCreate(ExecutionBase):
    pass

class ExecutionResponse(ExecutionBase):
    id: str
    started_by: str
    started_at: datetime
    finished_at: Optional[datetime]
    status: ExecutionStatus
    total_rows: Optional[int]
    total_rules: Optional[int]
    rows_affected: Optional[int]
    columns_affected: Optional[int]
    summary: Optional[str]  # JSON string
    model_config = ConfigDict(from_attributes=True)

# Issue schemas
class IssueBase(BaseModel):
    row_index: int
    column_name: str
    current_value: Optional[str] = None
    suggested_value: Optional[str] = None
    message: Optional[str] = None
    category: Optional[str] = None
    severity: Criticality

class IssueCreate(IssueBase):
    execution_id: str
    rule_id: str

class IssueResponse(IssueBase):
    id: str
    execution_id: str
    rule_id: str
    created_at: datetime
    resolved: bool
    model_config = ConfigDict(from_attributes=True)

# Fix schemas
class FixBase(BaseModel):
    new_value: Optional[str] = None
    comment: Optional[str] = None

class FixCreate(FixBase):
    issue_id: str

class FixResponse(FixBase):
    id: str
    issue_id: str
    fixed_by: str
    fixed_at: datetime
    model_config = ConfigDict(from_attributes=True)

# Export schemas
class ExportBase(BaseModel):
    dataset_version_id: str
    execution_id: Optional[str] = None
    format: ExportFormat
    location: Optional[str] = None

class ExportCreate(ExportBase):
    pass

class ExportResponse(ExportBase):
    id: str
    created_by: str
    created_at: datetime
    model_config = ConfigDict(from_attributes=True)

# File upload schemas
class FileUploadResponse(BaseModel):
    message: str
    filename: str
    size: int
    dataset_id: str
    
class DataProfileResponse(BaseModel):
    total_rows: int
    total_columns: int
    columns: List[DatasetColumnResponse]
    data_types_summary: dict
    missing_values_summary: dict
    
# Report schemas
class DataQualitySummary(BaseModel):
    total_issues: int
    critical_issues: int
    high_issues: int
    medium_issues: int
    low_issues: int
    resolved_issues: int
    categories_breakdown: dict

class ExecutionSummary(BaseModel):
    execution_id: str
    dataset_name: str
    started_at: datetime
    finished_at: Optional[datetime]
    status: ExecutionStatus
    total_rules_executed: int
    issues_found: int
    data_quality_summary: DataQualitySummary

# Rule testing schemas
class RuleTestRequest(BaseModel):
    sample_data: List[dict]
</file>

<file path="api/migrations/alembic/versions/04ca9eab8c98_create_employee_table.py">
"""Create employee table

Revision ID: 04ca9eab8c98
Revises: 
Create Date: 2025-07-26 15:30:00.861000

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '04ca9eab8c98'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    op.create_table("employee", 
                    sa.Column("Id", sa.Integer, primary_key=True), 
                    sa.Column("name", sa.String(50), nullable=False),
                    sa.Column("Current",sa.Boolean, default=True))


def downgrade() -> None:
    op.drop_table("employee")
</file>

<file path="api/migrations/alembic/versions/f3f72ff529c3_create_comprehensive_data_hygiene_schema.py">
"""Create comprehensive data hygiene schema

Revision ID: f3f72ff529c3
Revises: 04ca9eab8c98
Create Date: 2025-08-13 15:01:28.651569

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f3f72ff529c3'
down_revision: Union[str, None] = '04ca9eab8c98'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('users',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('email', sa.String(), nullable=False),
    sa.Column('role', postgresql.ENUM('admin', 'analyst', 'viewer', name='userrole'), nullable=False),
    sa.Column('auth_provider', sa.String(), nullable=True),
    sa.Column('auth_subject', sa.String(), nullable=True),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('email')
    )
    op.create_index(op.f('ix_users_id'), 'users', ['id'], unique=False)
    op.create_table('datasets',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('source_type', postgresql.ENUM('csv', 'excel', 'sap', 'ms_dynamics', 'other', name='sourcetype'), nullable=False),
    sa.Column('original_filename', sa.String(), nullable=True),
    sa.Column('checksum', sa.String(), nullable=True),
    sa.Column('uploaded_by', sa.String(), nullable=False),
    sa.Column('uploaded_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('status', postgresql.ENUM('uploaded', 'profiled', 'validated', 'cleaned', 'exported', name='datasetstatus'), nullable=True),
    sa.Column('row_count', sa.Integer(), nullable=True),
    sa.Column('column_count', sa.Integer(), nullable=True),
    sa.Column('notes', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['uploaded_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_datasets_id'), 'datasets', ['id'], unique=False)
    op.create_table('rules',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('kind', postgresql.ENUM('missing_data', 'standardization', 'value_list', 'length_range', 'cross_field', 'char_restriction', 'regex', 'custom', name='rulekind'), nullable=False),
    sa.Column('criticality', postgresql.ENUM('low', 'medium', 'high', 'critical', name='criticality'), nullable=False),
    sa.Column('is_active', sa.Boolean(), nullable=True),
    sa.Column('target_table', sa.String(), nullable=True),
    sa.Column('target_columns', sa.Text(), nullable=True),
    sa.Column('params', sa.Text(), nullable=True),
    sa.Column('created_by', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('updated_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('name')
    )
    op.create_index(op.f('ix_rules_id'), 'rules', ['id'], unique=False)
    op.create_table('dataset_columns',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('dataset_id', sa.String(), nullable=False),
    sa.Column('name', sa.String(), nullable=False),
    sa.Column('ordinal_position', sa.Integer(), nullable=False),
    sa.Column('inferred_type', sa.String(), nullable=True),
    sa.Column('is_nullable', sa.Boolean(), nullable=True),
    sa.ForeignKeyConstraint(['dataset_id'], ['datasets.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_dataset_columns_id'), 'dataset_columns', ['id'], unique=False)
    op.create_table('dataset_versions',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('dataset_id', sa.String(), nullable=False),
    sa.Column('version_no', sa.Integer(), nullable=False),
    sa.Column('created_by', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('rows', sa.Integer(), nullable=True),
    sa.Column('columns', sa.Integer(), nullable=True),
    sa.Column('change_note', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
    sa.ForeignKeyConstraint(['dataset_id'], ['datasets.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_dataset_versions_id'), 'dataset_versions', ['id'], unique=False)
    op.create_table('executions',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('dataset_version_id', sa.String(), nullable=False),
    sa.Column('started_by', sa.String(), nullable=False),
    sa.Column('started_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('finished_at', sa.DateTime(), nullable=True),
    sa.Column('status', postgresql.ENUM('queued', 'running', 'succeeded', 'failed', 'partially_succeeded', name='executionstatus'), nullable=True),
    sa.Column('total_rows', sa.Integer(), nullable=True),
    sa.Column('total_rules', sa.Integer(), nullable=True),
    sa.Column('rows_affected', sa.Integer(), nullable=True),
    sa.Column('columns_affected', sa.Integer(), nullable=True),
    sa.Column('summary', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['dataset_version_id'], ['dataset_versions.id'], ),
    sa.ForeignKeyConstraint(['started_by'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_executions_id'), 'executions', ['id'], unique=False)
    op.create_table('rule_columns',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('rule_id', sa.String(), nullable=False),
    sa.Column('column_id', sa.String(), nullable=False),
    sa.ForeignKeyConstraint(['column_id'], ['dataset_columns.id'], ),
    sa.ForeignKeyConstraint(['rule_id'], ['rules.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_rule_columns_id'), 'rule_columns', ['id'], unique=False)
    op.create_table('version_journal',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('dataset_version_id', sa.String(), nullable=False),
    sa.Column('event', sa.String(), nullable=False),
    sa.Column('rows_affected', sa.Integer(), nullable=True),
    sa.Column('columns_affected', sa.Integer(), nullable=True),
    sa.Column('details', sa.Text(), nullable=True),
    sa.Column('occurred_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['dataset_version_id'], ['dataset_versions.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_version_journal_id'), 'version_journal', ['id'], unique=False)
    op.create_table('execution_rules',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('execution_id', sa.String(), nullable=False),
    sa.Column('rule_id', sa.String(), nullable=False),
    sa.Column('error_count', sa.Integer(), nullable=True),
    sa.Column('rows_flagged', sa.Integer(), nullable=True),
    sa.Column('cols_flagged', sa.Integer(), nullable=True),
    sa.Column('note', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['execution_id'], ['executions.id'], ),
    sa.ForeignKeyConstraint(['rule_id'], ['rules.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_execution_rules_id'), 'execution_rules', ['id'], unique=False)
    op.create_table('exports',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('dataset_version_id', sa.String(), nullable=False),
    sa.Column('execution_id', sa.String(), nullable=True),
    sa.Column('format', postgresql.ENUM('csv', 'excel', 'json', 'api', 'datalake', name='exportformat'), nullable=False),
    sa.Column('location', sa.String(), nullable=True),
    sa.Column('created_by', sa.String(), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ),
    sa.ForeignKeyConstraint(['dataset_version_id'], ['dataset_versions.id'], ),
    sa.ForeignKeyConstraint(['execution_id'], ['executions.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_exports_id'), 'exports', ['id'], unique=False)
    op.create_table('issues',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('execution_id', sa.String(), nullable=False),
    sa.Column('rule_id', sa.String(), nullable=False),
    sa.Column('row_index', sa.Integer(), nullable=False),
    sa.Column('column_name', sa.String(), nullable=False),
    sa.Column('current_value', sa.Text(), nullable=True),
    sa.Column('suggested_value', sa.Text(), nullable=True),
    sa.Column('message', sa.Text(), nullable=True),
    sa.Column('category', sa.String(), nullable=True),
    sa.Column('severity', postgresql.ENUM('low', 'medium', 'high', 'critical', name='criticality'), nullable=False),
    sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('resolved', sa.Boolean(), nullable=True),
    sa.ForeignKeyConstraint(['execution_id'], ['executions.id'], ),
    sa.ForeignKeyConstraint(['rule_id'], ['rules.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_issues_id'), 'issues', ['id'], unique=False)
    op.create_table('fixes',
    sa.Column('id', sa.String(), nullable=False),
    sa.Column('issue_id', sa.String(), nullable=False),
    sa.Column('fixed_by', sa.String(), nullable=False),
    sa.Column('fixed_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
    sa.Column('new_value', sa.Text(), nullable=True),
    sa.Column('comment', sa.Text(), nullable=True),
    sa.ForeignKeyConstraint(['fixed_by'], ['users.id'], ),
    sa.ForeignKeyConstraint(['issue_id'], ['issues.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_fixes_id'), 'fixes', ['id'], unique=False)
    op.drop_table('employee')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('employee',
    sa.Column('Id', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('name', sa.VARCHAR(length=50), autoincrement=False, nullable=False),
    sa.Column('Current', sa.BOOLEAN(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('Id', name=op.f('employee_pkey'))
    )
    op.drop_index(op.f('ix_fixes_id'), table_name='fixes')
    op.drop_table('fixes')
    op.drop_index(op.f('ix_issues_id'), table_name='issues')
    op.drop_table('issues')
    op.drop_index(op.f('ix_exports_id'), table_name='exports')
    op.drop_table('exports')
    op.drop_index(op.f('ix_execution_rules_id'), table_name='execution_rules')
    op.drop_table('execution_rules')
    op.drop_index(op.f('ix_version_journal_id'), table_name='version_journal')
    op.drop_table('version_journal')
    op.drop_index(op.f('ix_rule_columns_id'), table_name='rule_columns')
    op.drop_table('rule_columns')
    op.drop_index(op.f('ix_executions_id'), table_name='executions')
    op.drop_table('executions')
    op.drop_index(op.f('ix_dataset_versions_id'), table_name='dataset_versions')
    op.drop_table('dataset_versions')
    op.drop_index(op.f('ix_dataset_columns_id'), table_name='dataset_columns')
    op.drop_table('dataset_columns')
    op.drop_index(op.f('ix_rules_id'), table_name='rules')
    op.drop_table('rules')
    op.drop_index(op.f('ix_datasets_id'), table_name='datasets')
    op.drop_table('datasets')
    op.drop_index(op.f('ix_users_id'), table_name='users')
    op.drop_table('users')
    # ### end Alembic commands ###
</file>

<file path="api/migrations/alembic/env.py">
from app import models  # Import all models so Alembic can detect them
from app.database import Base
from dotenv import load_dotenv
from alembic import context
from sqlalchemy import engine_from_config, pool
from logging.config import fileConfig
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

# Import Base and Models


# This is the Alembic Config object, which provides access to .ini values
config = context.config

# Detect if we're running in Docker container


def is_running_in_docker():
    return os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER') == 'true'


# Check if we're in Docker first
if is_running_in_docker():
    # In Docker - use environment variable directly
    docker_db_url = os.getenv("DATABASE_URL")
    if docker_db_url:
        config.set_main_option("sqlalchemy.url", docker_db_url)
        print(" Using DATABASE_URL from Docker environment:", docker_db_url)
    else:
        raise ValueError("DATABASE_URL not set in Docker environment")
else:
    # Local development - load .env file
    dotenv_loaded = load_dotenv(dotenv_path=".env.local", override=True)
    print(" .env.local loaded?", dotenv_loaded)
    local_db_url = os.getenv("DATABASE_URL")
    print(" DATABASE_URL:", local_db_url)
    if local_db_url:
        config.set_main_option("sqlalchemy.url", local_db_url)
    else:
        raise ValueError("DATABASE_URL not found in .env.local")

# Optional: enable Alembic logging if using logging config
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="api/migrations/alembic/README">
Generic single-database configuration.
</file>

<file path="api/migrations/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}
</file>

<file path="api/migrations/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts
# Use forward slashes (/) also on windows to provide an os agnostic path
script_location = alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.
prepend_sys_path = .

# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "version_path_separator" below.
# version_locations = %(here)s/bar:%(here)s/bat:alembic/versions

# version path separator; As mentioned above, this is the character used to split
# version_locations. The default within new alembic.ini files is "os", which uses os.pathsep.
# If this key is omitted entirely, it falls back to the legacy behavior of splitting on spaces and/or commas.
# Valid values for version_path_separator are:
#
# version_path_separator = :
# version_path_separator = ;
# version_path_separator = space
# version_path_separator = newline
version_path_separator = os  # Use os.pathsep. Default configuration used for new projects.

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

sqlalchemy.url = postgresql+psycopg://kshtj_test:password@postgres:5432/mytestdb


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the exec runner, execute a binary
# hooks = ruff
# ruff.type = exec
# ruff.executable = %(here)s/.venv/bin/ruff
# ruff.options = --fix REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path=".gitignore">
# OS/IDE
.DS_Store
.vscode/
.idea/

# Python
__pycache__/
*.py[oc]
*.egg-info/
.venv/
.env
.env.*

# Node/Next.js
node_modules/
.next/
dist/
out/
*.log
*.tsbuildinfo

# Angular (if ever used)
.angular/

# Docker
.dockerignore

# Build artifacts
build/
wheels/

# Misc
uv.lock
</file>

<file path=".python-version">
3.11
</file>

<file path="Dockerfile.fastapi">
FROM python:3.12-slim

WORKDIR /api

# Install uv (modern Python package manager)
RUN pip install uv

# Copy project files
COPY ../pyproject.toml .
COPY ../uv.lock .

RUN uv pip install -r pyproject.toml --system --compile-bytecode

# Volume mount for hot reload (uncomment for deployment)
# COPY app/ ./app   
ENV PYTHONPATH = ./api/app

# Expose port & run FastAPI
EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
</file>

<file path="Dockerfile.migrations">
FROM python:3.12-slim

WORKDIR /api

RUN pip install uv

COPY ../pyproject.toml .
COPY ../uv.lock .
RUN uv pip install -r pyproject.toml --system --compile-bytecode

COPY migrations/ ./migrations
COPY app/ ./app

# Set PYTHONPATH to include the current directory so imports work
ENV PYTHONPATH=/api 


#CMD ["alembic", "revision", "-m" ,"Create employee table"]
#CMD ["alembic", "upgrade", "head"]
</file>

<file path="pyproject.toml">
[project]
name = "api"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "alembic>=1.16.4",
    "celery>=5.5.3",
    "click>=8.2.1",
    "fastapi>=0.116.1",
    "h11>=0.16.0",
    "openpyxl>=3.1.5",
    "pandas>=2.3.1",
    "passlib>=1.7.4",
    "psycopg[binary]>=3.2.9",
    "pydantic[email]>=2.11.7",
    "python-dotenv>=1.1.1",
    "python-jose[cryptography]>=3.5.0",
    "python-multipart>=0.0.20",
    "redis>=6.4.0",
    "sqlalchemy>=2.0.41",
    "typing-extensions>=4.14.1",
    "uuid>=1.30",
    "uvicorn>=0.35.0",
]

[build-system]
requires = ["setuptools>=42", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
py-modules = []
</file>

<file path="UI_plan.md">
Phase 1: Foundation & Setup (Week 1)

    1.1 Project Foundation

    - Set up proper Tailwind CSS configuration with design system
    - Install and configure Shadcn/ui component library
    - Set up proper TypeScript types and utilities
    - Configure ESLint, Prettier for code quality

    1.2 Design System Recreation

    - Implement comprehensive CSS custom properties for data quality themes
    - Set up responsive typography system (Inter + JetBrains Mono)
    - Create color palette for data quality semantic meanings (clean, warning, error, info)
    - Implement dark/light mode support

    Phase 2: Authentication System (Week 1-2)

    2.1 NextAuth.js Integration

    - Configure NextAuth.js v5 with credentials provider
    - Set up JWT strategy with FastAPI backend integration
    - Create login/logout pages with form validation
    - Implement session management and route protection

    2.2 Authentication Components

    - Login form with demo one-click login
    - User profile dropdown in header
    - Protected route middleware
    - Error handling and redirects

    Phase 3: Core Layout & Navigation (Week 2)

    3.1 App Layout Structure

    - Responsive main layout with sidebar
    - Professional header with search, notifications, user menu
    - Collapsible sidebar with role-based navigation
    - Mobile-responsive design with overlay

    3.2 Navigation System

    - Hierarchical navigation with expandable groups
    - Route-based active states
    - Quick stats in sidebar
    - Theme toggle integration

    Phase 4: Dashboard Implementation (Week 2-3)

    4.1 Dashboard Components

    - KPI cards with trending indicators
    - Data quality metrics with progress bars
    - Recent activity timeline
    - Quick action buttons

    4.2 Data Visualization

    - Quality score breakdown charts
    - Issue distribution visualizations
    - Trend analysis graphs using Recharts
    - Interactive filtering and drilling

    Phase 5: Data Management Features (Week 3-4)

    5.1 File Upload System

    - Drag-and-drop file upload component
    - Progress tracking and validation
    - File type restrictions and error handling
    - Integration with FastAPI upload endpoints

    5.2 Dataset Management

    - Dataset listing with advanced filtering
    - Dataset detail views with column analysis
    - Data grid with virtualization for large datasets
    - Export functionality

    Phase 6: Rules & Quality Management (Week 4-5)

    6.1 Rule Builder Interface

    - Visual rule creation form
    - Condition builder with drag-and-drop
    - Rule testing and validation
    - Template library for common rules

    6.2 Execution Management

    - Rule execution dashboard
    - Real-time execution monitoring
    - Results visualization and analysis
    - Issue management and resolution tracking

    Phase 7: Advanced Features (Week 5-6)

    7.1 Reports & Analytics

    - Automated report generation
    - Custom report builder
    - Scheduled reporting
    - Export to multiple formats

    7.2 Administration

    - User management (if admin role)
    - System settings and configuration
    - Audit logs and activity tracking
    - Performance monitoring

    Phase 8: Testing & Optimization (Week 6)

    8.1 Quality Assurance

    - Unit tests for components
    - Integration tests for API connections
    - E2E tests for critical user flows
    - Accessibility compliance testing

    8.2 Performance & Polish

    - Code splitting and lazy loading
    - Bundle optimization
    - Error boundaries and fallbacks
    - Final UI polish and animations

    Technology Stack

    - Framework: Next.js 15 with App Router
    - Styling: Tailwind CSS v4 with custom design system
    - Components: Shadcn/ui + custom components
    - Authentication: NextAuth.js v5
    - API: React Query for data fetching
    - State: Zustand for global state management
    - Forms: React Hook Form + Zod validation
    - Charts: Recharts for data visualization
    - Icons: Lucide React

    Success Criteria

    1. Functional: All CRUD operations working with FastAPI backend
    2. Responsive: Mobile-first design working on all devices
    3. Accessible: WCAG 2.1 AA compliance
    4. Performance: < 3s load time, smooth interactions
    5. Professional: Clean, modern UI matching data quality domain
</file>

</files>
