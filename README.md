# 🧹 Data Hygiene Toolkit

A comprehensive data quality validation and management platform for ensuring data integrity across your organization. Built with FastAPI, Next.js, and PostgreSQL.

[![FastAPI](https://img.shields.io/badge/FastAPI-0.116+-green.svg)](https://fastapi.tiangolo.com/)
[![Next.js](https://img.shields.io/badge/Next.js-15.5-black.svg)](https://nextjs.org/)
[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5+-blue.svg)](https://www.typescriptlang.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## 📋 Table of Contents

- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [API Documentation](#-api-documentation)
- [Database Schema](#-database-schema)
- [Rule Types](#-rule-types)
- [Deployment](#-deployment)
- [Performance Optimization](#-performance-optimization)
- [Contributing](#-contributing)
- [Troubleshooting](#-troubleshooting)
- [License](#-license)

---

## ✨ Features

### Core Capabilities
- 📊 **Multi-Format Data Import**: CSV, Excel, Parquet support
- 🎯 **Comprehensive Rule Engine**: 8 validator types for data quality
- 📈 **Real-time Validation**: Execute rules on datasets with detailed issue tracking
- 📝 **Rule Versioning**: Track changes with complete audit trails
- 🔄 **Batch Processing**: Handle large datasets with memory-efficient chunking
- 📤 **Export Capabilities**: Export validated data in multiple formats
- 🔍 **Advanced Search**: Full-text search across datasets, rules, and issues
- 👥 **Role-Based Access**: Admin, Analyst, and Viewer roles
- 📊 **Analytics Dashboard**: Visual insights into data quality metrics

### Data Quality Features
- **Missing Data Detection**: Identify null/empty values in critical fields
- **Standardization**: Enforce date, phone, email format consistency
- **Value List Validation**: Restrict values to predefined lists
- **Length Constraints**: Min/max character length validation
- **Cross-Field Rules**: Complex relationships between multiple fields
- **Character Restrictions**: Alphabetic, numeric, alphanumeric validation
- **Regex Patterns**: Custom pattern matching validation
- **Custom Validation**: Python expressions for complex business logic

### Technical Features
- 🚀 **Optimized Performance**: Connection pooling and memory management
- 🔒 **Secure Authentication**: OAuth/JWT-based auth with NextAuth
- 📱 **Responsive UI**: Modern design with shadcn/ui components
- 🎨 **Dark Mode**: Built-in theme support
- 📊 **Visual Reports**: Charts and graphs with Recharts
- 🔄 **Real-time Updates**: React Query for data synchronization
- 🐳 **Docker Support**: Complete containerization for easy deployment

---

## 🛠 Tech Stack

### Backend
- **Framework**: FastAPI 0.116+
- **Language**: Python 3.11+
- **Database**: PostgreSQL 16
- **ORM**: SQLAlchemy 2.0+
- **Migrations**: Alembic
- **Data Processing**: Pandas, PyArrow
- **Validation**: Pydantic 2.0+
- **Authentication**: python-jose, passlib

### Frontend
- **Framework**: Next.js 15.5 (App Router)
- **Language**: TypeScript 5+
- **UI Components**: Radix UI, shadcn/ui
- **Styling**: Tailwind CSS 4
- **State Management**: Zustand, React Query
- **Data Visualization**: Recharts
- **3D Graphics**: React Three Fiber (optional)
- **Authentication**: NextAuth.js

### Infrastructure
- **API Hosting**: Fly.io
- **Frontend Hosting**: Vercel
- **Database**: Supabase (PostgreSQL)
- **File Storage**: Local filesystem / Volume mounts
- **Containerization**: Docker & Docker Compose

---

## 🏗 Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                        Frontend (Vercel)                    │
│  Next.js 15 + TypeScript + Tailwind + shadcn/ui             │
└────────────────────────┬────────────────────────────────────┘
                         │
                         │ HTTPS/REST
                         │
┌────────────────────────▼────────────────────────────────────┐
│                     Backend API (Fly.io)                    │
│             FastAPI + Python 3.11 + SQLAlchemy              │
│                                                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │
│  │   Routes     │  │   Services   │  │    Models    │       │
│  │  - Auth      │  │  - Rule Eng  │  │  - Dataset   │       │
│  │  - Upload    │  │  - Data Imp  │  │  - Rule      │       │
│  │  - Rules     │  │  - Versioning│  │  - Execution │       │
│  │  - Execute   │  │  - Export    │  │  - Issue     │       │
│  └──────────────┘  └──────────────┘  └──────────────┘       │
└────────────────────────┬────────────────────────────────────┘
                         │
                         │ SQL
                         │
┌────────────────────────▼────────────────────────────────────┐
│                 Database (Supabase)                         │
│           PostgreSQL 16 + Automatic Backups                 │
└─────────────────────────────────────────────────────────────┘
```

### Component Breakdown

#### API Layer (`/api`)
```
api/
├── app/
│   ├── routes/          # API endpoints
│   │   ├── auth/        # Authentication
│   │   ├── upload/      # File uploads
│   │   ├── rules.py     # Rule management
│   │   ├── executions.py# Rule execution
│   │   ├── issues.py    # Issue management
│   │   ├── reports.py   # Analytics
│   │   └── search.py    # Search functionality
│   ├── services/        # Business logic
│   │   ├── rule_engine.py      # Validation engine
│   │   ├── data_import.py      # File processing
│   │   ├── rule_versioning.py  # Version control
│   │   └── export.py           # Data export
│   ├── utils/           # Utilities
│   │   └── memory_optimization.py  # Performance
│   ├── models.py        # Database models
│   ├── schemas.py       # Pydantic schemas
│   ├── database.py      # DB connection
│   └── main.py          # FastAPI app
├── migrations/          # Alembic migrations
└── data/               # File storage
```

#### Frontend Layer (`/frontend`)
```
frontend/
├── src/
│   ├── app/            # Next.js App Router pages
│   ├── components/     # React components
│   ├── lib/            # Utilities
│   └── hooks/          # Custom hooks
└── public/             # Static assets
```

---

## 🚀 Quick Start

### Prerequisites
- **Docker & Docker Compose** (recommended)
- OR
- **Python 3.11+** and **Node.js 18+**
- **PostgreSQL 16** (if running locally)

### Option 1: Docker (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd API

# Create environment file
cp .env.example .env
# Edit .env with your database credentials

# Start all services
docker-compose up -d

# Run migrations
docker-compose exec api alembic upgrade head

# Access the application
# API: http://localhost:8000
# Frontend: http://localhost:3000
# API Docs: http://localhost:8000/docs
```

### Option 2: Manual Setup

See [Installation](#-installation) section below.

---

## 📦 Installation

### Backend Setup

1. **Clone and navigate to project**
```bash
git clone <repository-url>
cd API
```

2. **Set up Python environment**
```bash
cd api

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install uv  # Fast Python package installer
uv pip install -e .
```

3. **Configure environment**
```bash
# Copy example env file
cp ../.env.example ../.env

# Edit .env with your values:
# DATABASE_URL=postgresql://user:pass@localhost:5432/data_hygiene
# ASYNC_DATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/data_hygiene
```

4. **Set up database**
```bash
# Start PostgreSQL (if using Docker)
docker run -d \
  --name postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=data_hygiene \
  -p 5432:5432 \
  postgres:16-alpine

# Run migrations
alembic upgrade head
```

5. **Start the API**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Setup

1. **Navigate to frontend directory**
```bash
cd ../frontend
```

2. **Install dependencies**
```bash
npm install
# or
pnpm install
# or
yarn install
```

3. **Configure environment**
```bash
# Copy example env file
cp .env.example .env.local

# Edit .env.local:
# NEXT_PUBLIC_API_URL=http://localhost:8000
# NEXTAUTH_URL=http://localhost:3000
# NEXTAUTH_SECRET=your-secret-key
```

4. **Start development server**
```bash
npm run dev
```

5. **Access the application**
- Frontend: http://localhost:3000
- API: http://localhost:8000
- API Docs: http://localhost:8000/docs

---

## ⚙️ Configuration

### Environment Variables

#### Backend (`.env`)
```bash
# Database
DATABASE_URL=postgresql://user:password@host:5432/database
ASYNC_DATABASE_URL=postgresql+asyncpg://user:password@host:5432/database

# JWT/Auth (optional)
SECRET_KEY=your-secret-key-here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# CORS (adjust for production)
CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# File Storage
DATA_STORAGE_PATH=/app/data

# Performance
MAX_UPLOAD_SIZE=100MB
CHUNK_SIZE=10000
MEMORY_THRESHOLD_MB=150
```

#### Frontend (`.env.local`)
```bash
# API
NEXT_PUBLIC_API_URL=http://localhost:8000

# Authentication
NEXTAUTH_URL=http://localhost:3000
NEXTAUTH_SECRET=generate-with-openssl-rand-base64-32

# OAuth Providers (optional)
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret
```

### Database Configuration

**Fly.io Connection**
```toml
# fly.toml
[[vm]]
  memory = '512mb'  # Recommended minimum
  cpu_kind = 'shared'
  cpus = 1
```

**Connection Pool Settings** (`api/app/database.py`)
```python
POOL_CONFIG = {
    'pool_size': 3,              # Persistent connections
    'max_overflow': 5,           # Extra connections (total max = 8)
    'pool_timeout': 30,          # Wait timeout
    'pool_recycle': 1800,        # Recycle after 30 mins
    'pool_pre_ping': True,       # Health checks
}
```

---

## 📚 API Documentation

### Interactive Documentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Core Endpoints

#### Authentication
```
POST   /auth/login          # Login user
POST   /auth/register       # Register new user
POST   /auth/token          # Get access token
GET    /auth/me             # Get current user
```

#### Datasets
```
POST   /upload/             # Upload new dataset
GET    /datasets/           # List all datasets
GET    /datasets/{id}       # Get dataset details
DELETE /datasets/{id}       # Delete dataset
GET    /datasets/{id}/profile  # Get data profile
```

#### Rules
```
GET    /rules/              # List all rules
POST   /rules/              # Create new rule
GET    /rules/{id}          # Get rule details
PUT    /rules/{id}          # Update rule
DELETE /rules/{id}          # Soft delete rule
POST   /rules/{id}/duplicate # Duplicate rule
GET    /rules/{id}/history  # Get rule version history
POST   /rules/{id}/rollback # Rollback to previous version
```

#### Executions
```
POST   /executions/         # Execute rules on dataset
GET    /executions/         # List all executions
GET    /executions/{id}     # Get execution details
GET    /executions/{id}/issues  # Get execution issues
```

#### Issues
```
GET    /issues/             # List all issues
GET    /issues/{id}         # Get issue details
PUT    /issues/{id}         # Update issue
POST   /issues/{id}/fix     # Mark issue as fixed
POST   /issues/bulk-fix     # Bulk fix issues
```

#### Reports & Analytics
```
GET    /reports/summary     # Get summary statistics
GET    /reports/trends      # Get trend data
GET    /reports/quality-score  # Calculate quality score
```

#### Search
```
GET    /search/datasets     # Search datasets
GET    /search/rules        # Search rules
GET    /search/issues       # Search issues
GET    /search/             # Global search
```

### Request Examples

**Upload Dataset**
```bash
curl -X POST "http://localhost:8000/upload/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@data.csv" \
  -F "dataset_name=Customer Data"
```

**Create Rule**
```bash
curl -X POST "http://localhost:8000/rules/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Email Validation",
    "description": "Validate email format",
    "kind": "standardization",
    "criticality": "high",
    "target_columns": ["email"],
    "params": {
      "type": "email"
    }
  }'
```

**Execute Rules**
```bash
curl -X POST "http://localhost:8000/executions/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "dataset_version_id": "dataset-uuid",
    "rule_ids": ["rule-uuid-1", "rule-uuid-2"]
  }'
```

---

## 🗄️ Database Schema

### Core Tables

#### Users
```sql
users (
  id: UUID PRIMARY KEY,
  name: STRING,
  email: STRING UNIQUE,
  role: ENUM(admin, analyst, viewer),
  auth_provider: STRING,
  auth_subject: STRING,
  created_at: TIMESTAMP,
  updated_at: TIMESTAMP
)
```

#### Datasets
```sql
datasets (
  id: UUID PRIMARY KEY,
  name: STRING,
  source_type: ENUM(csv, excel, sap, ms_dynamics, other),
  original_filename: STRING,
  checksum: STRING,
  uploaded_by: UUID FOREIGN KEY -> users.id,
  uploaded_at: TIMESTAMP,
  status: ENUM(uploaded, profiled, validated, cleaned, exported),
  row_count: INTEGER,
  column_count: INTEGER,
  notes: TEXT
)
```

#### Rules
```sql
rules (
  id: UUID PRIMARY KEY,
  name: STRING,
  description: TEXT,
  kind: ENUM(missing_data, standardization, value_list, ...),
  criticality: ENUM(low, medium, high, critical),
  is_active: BOOLEAN,
  target_columns: JSON,
  params: JSON,
  version: INTEGER,
  parent_rule_id: UUID FOREIGN KEY -> rules.id,
  is_latest: BOOLEAN,
  created_by: UUID FOREIGN KEY -> users.id,
  created_at: TIMESTAMP,
  updated_at: TIMESTAMP
)
```

#### Executions
```sql
executions (
  id: UUID PRIMARY KEY,
  dataset_version_id: UUID FOREIGN KEY,
  started_by: UUID FOREIGN KEY -> users.id,
  status: ENUM(queued, running, succeeded, failed, partially_succeeded),
  started_at: TIMESTAMP,
  finished_at: TIMESTAMP,
  total_rules: INTEGER,
  total_rows: INTEGER,
  rows_affected: INTEGER,
  columns_affected: INTEGER,
  summary: JSON
)
```

#### Issues
```sql
issues (
  id: UUID PRIMARY KEY,
  execution_id: UUID FOREIGN KEY -> executions.id,
  rule_id: UUID FOREIGN KEY -> rules.id,
  row_index: INTEGER,
  column_name: STRING,
  current_value: STRING,
  suggested_value: STRING,
  message: TEXT,
  category: STRING,
  severity: ENUM(low, medium, high, critical),
  is_fixed: BOOLEAN,
  fixed_at: TIMESTAMP,
  fixed_by: UUID FOREIGN KEY -> users.id
)
```

### Relationships
```
users (1) ----< (*) datasets
users (1) ----< (*) rules
users (1) ----< (*) executions
datasets (1) ----< (*) dataset_versions
dataset_versions (1) ----< (*) executions
executions (1) ----< (*) issues
rules (1) ----< (*) issues
rules (1) ----< (*) version_journal (for audit trail)
```

---

## 🎯 Rule Types

### 1. Missing Data Validator
Detects null, empty, or missing values in required fields.

```json
{
  "kind": "missing_data",
  "target_columns": ["email", "phone"],
  "params": {
    "default_value": ""
  }
}
```

### 2. Standardization Validator
Enforces format consistency for dates, phones, emails.

```json
{
  "kind": "standardization",
  "target_columns": ["order_date"],
  "params": {
    "type": "date",
    "format": "%Y-%m-%d"
  }
}
```

### 3. Value List Validator
Restricts values to predefined allowed lists.

```json
{
  "kind": "value_list",
  "target_columns": ["status"],
  "params": {
    "allowed_values": ["active", "inactive", "pending"],
    "case_sensitive": false
  }
}
```

### 4. Length Range Validator
Validates min/max character length.

```json
{
  "kind": "length_range",
  "target_columns": ["zip_code"],
  "params": {
    "min_length": 5,
    "max_length": 10
  }
}
```

### 5. Cross-Field Validator
Complex relationships between multiple fields.

```json
{
  "kind": "cross_field",
  "params": {
    "rules": [
      {
        "type": "dependency",
        "dependent_field": "shipping_address",
        "required_field": "shipping_method"
      }
    ]
  }
}
```

### 6. Character Restriction Validator
Validates allowed character types.

```json
{
  "kind": "char_restriction",
  "target_columns": ["customer_name"],
  "params": {
    "type": "alphabetic"
  }
}
```

### 7. Regex Validator
Custom pattern matching validation.

```json
{
  "kind": "regex",
  "target_columns": ["product_code"],
  "params": {
    "patterns": [
      {
        "name": "Product Code Format",
        "pattern": "^[A-Z]{3}-\\d{4}$",
        "must_match": true
      }
    ]
  }
}
```

### 8. Custom Validator
Python expression-based validation.

```json
{
  "kind": "custom",
  "target_columns": ["price", "discount"],
  "params": {
    "type": "python_expression",
    "expression": "price > 0 and discount < price",
    "error_message": "Invalid price/discount combination"
  }
}
```

---

## 🚢 Deployment

### Fly.io (Backend API)

1. **Install Fly CLI**
```bash
curl -L https://fly.io/install.sh | sh
```

2. **Login and create app**
```bash
fly auth login
fly launch
```

3. **Configure fly.toml**
```toml
app = "data-hygiene-toolkit"
primary_region = "iad"

[build]

[deploy]
  release_command = "alembic upgrade head"

[env]
  PORT = "8000"

[[vm]]
  memory = '512mb'  # Recommended: 512MB or 1GB
  cpu_kind = 'shared'
  cpus = 1

[http_service]
  internal_port = 8000
  force_https = true
```

4. **Set secrets**
```bash
fly secrets set DATABASE_URL="postgresql://..."
fly secrets set SECRET_KEY="your-secret"
```

5. **Deploy**
```bash
fly deploy
```

### Vercel (Frontend)

1. **Install Vercel CLI**
```bash
npm i -g vercel
```

2. **Deploy**
```bash
cd frontend
vercel
```

3. **Configure environment variables in Vercel Dashboard**
- `NEXT_PUBLIC_API_URL`
- `NEXTAUTH_URL`
- `NEXTAUTH_SECRET`
- OAuth credentials (if using)

### Supabase (Database)

1. **Create project** at https://supabase.com
2. **Get connection string** from Settings > Database
3. **Run migrations**
```bash
# Point to Supabase database
DATABASE_URL="postgresql://..." alembic upgrade head
```

---

## ⚡ Performance Optimization

### Connection Pool Optimization

The API includes optimized connection pooling for memory-constrained environments:

```python
# Configured in api/app/database.py
POOL_CONFIG = {
    'pool_size': 3,          # Reduced persistent connections
    'max_overflow': 5,       # Maximum 8 total connections
    'pool_timeout': 30,      # Connection wait timeout
    'pool_recycle': 1800,    # Recycle connections every 30 mins
    'pool_pre_ping': True,   # Verify connection health
}
```

**Benefits:**
- 40-50% reduction in connection overhead
- Better stability under load
- No connection pool exhaustion

### Memory Optimization

For handling large datasets efficiently:

```python
from app.utils import ChunkedDataFrameReader, MemoryMonitor

# Initialize chunked reader
reader = ChunkedDataFrameReader(chunk_size=10000)

# Monitor memory usage
MemoryMonitor.log_memory_usage("before processing")

# Process in chunks
for chunk in reader.read_csv_chunked(file_content):
    process_chunk(chunk)

MemoryMonitor.log_memory_usage("after processing")
```

**Features:**
- Automatic chunking for files >50MB
- Memory usage monitoring
- Dtype optimization (30-60% memory savings)
- Handles files 5-10x larger with same memory

**Configuration:**
```python
# Adjust chunk size based on available memory
ChunkedDataFrameReader(
    chunk_size=10000,        # Rows per chunk
    memory_threshold_mb=150  # Memory threshold
)
```

### Performance Metrics

| Metric | Before Optimization | After Optimization |
|--------|-------------------|-------------------|
| **Memory (256MB VM)** | OOM on 5MB+ files | Handles 30-40MB files |
| **Connection Pool** | 5-15 connections | 3-8 connections |
| **File Processing** | 50MB CSV = 125MB+ RAM | 50MB CSV = 80-100MB RAM |
| **API Response** | Frequent timeouts | <2s for most requests |

---

## 🤝 Contributing

Contributions are welcome! Please follow these guidelines:

### Development Workflow

1. **Fork the repository**
2. **Create a feature branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```
3. **Make your changes**
4. **Test thoroughly**
   ```bash
   # Backend tests
   cd api
   pytest
   
   # Frontend tests
   cd frontend
   npm test
   ```
5. **Commit with conventional commits**
   ```bash
   git commit -m "feat: add amazing feature"
   ```
6. **Push and create PR**
   ```bash
   git push origin feature/amazing-feature
   ```

### Code Standards

**Backend (Python)**
- Follow PEP 8
- Use type hints
- Add docstrings for functions/classes
- Write unit tests with pytest

**Frontend (TypeScript)**
- Follow ESLint rules
- Use TypeScript strict mode
- Component-based architecture
- Write unit tests with Jest/React Testing Library

### Commit Message Convention
```
feat: Add new feature
fix: Fix bug
docs: Update documentation
style: Code style changes
refactor: Code refactoring
test: Add tests
chore: Build/tooling changes
```

---

## 🔧 Troubleshooting

### Common Issues

#### 1. Database Connection Errors
```bash
# Check database URL
echo $DATABASE_URL

# Test connection
psql $DATABASE_URL

# Reset migrations
alembic downgrade base
alembic upgrade head
```

#### 2. Memory Issues (OOM Errors)
```bash
# Check Fly.io memory
fly status

# Increase memory in fly.toml
[[vm]]
  memory = '512mb'  # or '1gb'

# Deploy changes
fly deploy
```

#### 3. Frontend API Connection
```bash
# Check API URL in .env.local
echo $NEXT_PUBLIC_API_URL

# Test API health
curl http://localhost:8000/

# Check CORS settings in api/app/main.py
```

#### 4. File Upload Failures
```bash
# Check file size limits
# Adjust MAX_UPLOAD_SIZE in .env

# Verify storage path exists
mkdir -p api/data/datasets

# Check permissions
chmod 755 api/data
```

#### 5. Rule Execution Timeout
```python
# Increase chunk size in memory_optimization.py
ChunkedDataFrameReader(chunk_size=5000)  # Reduce from 10000

# Or optimize DataFrame dtypes
from app.utils import OptimizedDataFrameOperations
df = OptimizedDataFrameOperations.optimize_dtypes(df)
```

### Debug Mode

**Enable debug logging:**
```python
# In api/app/main.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Check connection pool:**
```python
from app.database import get_pool_status, log_pool_status
log_pool_status()
```

**Monitor memory:**
```python
from app.utils import MemoryMonitor
MemoryMonitor.log_memory_usage("checkpoint")
```

---

## 📄 License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## 👤 Author

**Kshitij Bharambe**
- GitHub: [@kshitij-bharambe](https://github.com/kshitij-bharambe)
- Portfolio: [kshitij.space](https://kshitij.space)
- LinkedIn: [Kshitij Bharambe](https://linkedin.com/in/kshitij-bharambe)

---

## 🙏 Acknowledgments

- FastAPI for the excellent web framework
- Next.js team for the amazing React framework
- shadcn for the beautiful UI components
- Supabase for the database hosting
- Fly.io for the API hosting platform

---

## 📊 Project Status

- ✅ **Core Features**: Complete
- ✅ **API**: Stable
- ✅ **Frontend**: Production-ready
- ✅ **Performance**: Optimized
- 🔄 **Documentation**: Continuous improvement
- 🚀 **Active Development**: Regular updates

---

## 📞 Support

For issues, questions, or contributions:
1. Check [Troubleshooting](#-troubleshooting) section
2. Search [existing issues](https://github.com/your-repo/issues)
3. Create a [new issue](https://github.com/your-repo/issues/new)

---

**Built with ❤️ for better data quality**
